{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 284,
      "id": "74264a10",
      "metadata": {
        "id": "74264a10"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "seed_value = 42\n",
        "torch.manual_seed(seed_value)\n",
        "random.seed(seed_value)"
      ],
      "metadata": {
        "id": "Jf2Gb8DBUh8B"
      },
      "id": "Jf2Gb8DBUh8B",
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "id": "uBJOxAvKIXam",
      "metadata": {
        "id": "uBJOxAvKIXam"
      },
      "outputs": [],
      "source": [
        "# Import Data\n",
        "data = pd.read_csv('dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "id": "c4c380d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "c4c380d0",
        "outputId": "cbc384ba-99b5-44ab-d0ff-ccd41cf3e1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(766, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  f1   f2  f3  f4   f5    f6     f7  target\n",
              "0  6  148  72  35    0  33.6  0.627       1\n",
              "1  1   85  66  29    0  26.6  0.351       0\n",
              "2  8  183  64   0    0  23.3  0.672       1\n",
              "3  1   89  66  23   94  28.1  0.167       0\n",
              "4  0  137  40  35  168  43.1  2.288       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2213add6-5849-485b-82e4-523a9d79a7d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2213add6-5849-485b-82e4-523a9d79a7d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2213add6-5849-485b-82e4-523a9d79a7d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2213add6-5849-485b-82e4-523a9d79a7d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ],
      "source": [
        "# dropping NA values\n",
        "df = data.dropna()\n",
        "print(df.shape)\n",
        "df[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "id": "34ssvAHwFQ-p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "34ssvAHwFQ-p",
        "outputId": "0c501feb-38fa-490e-a189-279ef3e01a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(760, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  f1   f2  f3  f4   f5    f6     f7  target\n",
              "0  6  148  72  35    0  33.6  0.627       1\n",
              "1  1   85  66  29    0  26.6  0.351       0\n",
              "2  8  183  64   0    0  23.3  0.672       1\n",
              "3  1   89  66  23   94  28.1  0.167       0\n",
              "4  0  137  40  35  168  43.1  2.288       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e279686-abe0-4fe1-99c9-8f35499da47c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e279686-abe0-4fe1-99c9-8f35499da47c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e279686-abe0-4fe1-99c9-8f35499da47c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e279686-abe0-4fe1-99c9-8f35499da47c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ],
      "source": [
        "# drop non-numeric values from other attributes\n",
        "\n",
        "df = df.drop(df[df['f1'] == 'c'].index)\n",
        "df = df.drop(df[df['f2'] == 'f'].index)\n",
        "df = df.drop(df[df['f4'] == 'a'].index)\n",
        "df = df.drop(df[df['f5'] == 'b'].index)\n",
        "df = df.drop(df[df['f6'] == 'd'].index)\n",
        "df = df.drop(df[df['f7'] == 'e'].index)\n",
        "\n",
        "print(df.shape)\n",
        "df[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "id": "c66dc2ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c66dc2ca",
        "outputId": "5180e0c6-762f-43d5-a906-b8b516ef4fd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f1        f2        f3        f4        f5        f6        f7  \\\n",
              "0  0.352941  0.743719  0.590164  0.353535  0.000000  0.500745  0.234415   \n",
              "1  0.058824  0.427136  0.540984  0.292929  0.000000  0.396423  0.116567   \n",
              "2  0.470588  0.919598  0.524590  0.000000  0.000000  0.347243  0.253629   \n",
              "3  0.058824  0.447236  0.540984  0.232323  0.111111  0.418778  0.038002   \n",
              "4  0.000000  0.688442  0.327869  0.353535  0.198582  0.642325  0.943638   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       0  \n",
              "2       1  \n",
              "3       0  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35f221c5-0b7d-4f0c-ae4d-1bf359dcb22b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.743719</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500745</td>\n",
              "      <td>0.234415</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.427136</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.396423</td>\n",
              "      <td>0.116567</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.919598</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.347243</td>\n",
              "      <td>0.253629</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.447236</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.418778</td>\n",
              "      <td>0.038002</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688442</td>\n",
              "      <td>0.327869</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.198582</td>\n",
              "      <td>0.642325</td>\n",
              "      <td>0.943638</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35f221c5-0b7d-4f0c-ae4d-1bf359dcb22b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-35f221c5-0b7d-4f0c-ae4d-1bf359dcb22b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-35f221c5-0b7d-4f0c-ae4d-1bf359dcb22b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ],
      "source": [
        "# Scale numerical variables to have zero mean and unit variance.\n",
        "# use StandardScaler\n",
        "\n",
        "#scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "input_features = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7']\n",
        "\n",
        "# transform\n",
        "df_num = df[input_features]\n",
        "scaler.fit(df_num)\n",
        "scaled_data = scaler.transform(df_num) \n",
        "\n",
        "# create scaled DF\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=input_features)\n",
        "scaled_df.head()\n",
        "\n",
        "# replace original numerical columns in the original DF with scaled ones\n",
        "df[input_features] = scaled_df\n",
        "df.head()\n",
        "\n",
        "# pre-processing done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "id": "4e94fa90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "4e94fa90",
        "outputId": "7fc80059-6ec5-4595-a7dc-3caa1577f586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X: (760, 7)\n",
            "Size of y: (760,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f1        f2        f3        f4        f5        f6        f7\n",
              "0  0.352941  0.743719  0.590164  0.353535  0.000000  0.500745  0.234415\n",
              "1  0.058824  0.427136  0.540984  0.292929  0.000000  0.396423  0.116567\n",
              "2  0.470588  0.919598  0.524590  0.000000  0.000000  0.347243  0.253629\n",
              "3  0.058824  0.447236  0.540984  0.232323  0.111111  0.418778  0.038002\n",
              "4  0.000000  0.688442  0.327869  0.353535  0.198582  0.642325  0.943638"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57c16a19-d2f7-4acf-85b2-b3e8598b218b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.743719</td>\n",
              "      <td>0.590164</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500745</td>\n",
              "      <td>0.234415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.427136</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.292929</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.396423</td>\n",
              "      <td>0.116567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.919598</td>\n",
              "      <td>0.524590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.347243</td>\n",
              "      <td>0.253629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.447236</td>\n",
              "      <td>0.540984</td>\n",
              "      <td>0.232323</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.418778</td>\n",
              "      <td>0.038002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688442</td>\n",
              "      <td>0.327869</td>\n",
              "      <td>0.353535</td>\n",
              "      <td>0.198582</td>\n",
              "      <td>0.642325</td>\n",
              "      <td>0.943638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57c16a19-d2f7-4acf-85b2-b3e8598b218b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57c16a19-d2f7-4acf-85b2-b3e8598b218b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57c16a19-d2f7-4acf-85b2-b3e8598b218b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ],
      "source": [
        "# define Inputs and Output\n",
        "\n",
        "X_in = df[input_features]\n",
        "y_out = df['target']\n",
        "print('Size of X:', X_in.shape)\n",
        "print('Size of y:', y_out.shape)\n",
        "X_in.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "id": "0411edbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0411edbf",
        "outputId": "374909f6-146c-4eb0-84a9-e130a849433e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train size: (608, 7)\n",
            "y_train size: (608,)\n",
            "X_test size: (152, 7)\n",
            "y_test size: (152,)\n",
            "           f1        f2        f3        f4        f5        f6        f7\n",
            "693  0.411765  0.648241  0.557377  0.494949  0.147754  0.573770  0.154142\n",
            "361  0.294118  0.793970  0.573770  0.000000  0.000000  0.444113  0.055081\n",
            "266  0.000000  0.693467  0.000000  0.000000  0.000000  0.540984  0.365073\n",
            "90   0.058824  0.402010  0.450820  0.000000  0.000000  0.284650  0.076857\n",
            "368  0.176471  0.407035  0.704918  0.161616  0.078014  0.409836  0.097353\n",
            "           f1        f2        f3        f4        f5        f6        f7\n",
            "395  0.117647  0.638191  0.475410  0.242424  0.325059  0.412817  0.649872\n",
            "324  0.117647  0.562814  0.614754  0.323232  0.000000  0.532042  0.029889\n",
            "97   0.058824  0.356784  0.393443  0.181818  0.089835  0.304024  0.104611\n",
            "497  0.117647  0.407035  0.590164  0.151515  0.089835  0.448584  0.200256\n",
            "109  0.000000  0.477387  0.696721  0.252525  0.042553  0.557377  0.072161\n",
            "693    1\n",
            "361    0\n",
            "266    1\n",
            "90     0\n",
            "368    0\n",
            "Name: target, dtype: int64\n",
            "395    0\n",
            "324    0\n",
            "97     0\n",
            "497    0\n",
            "109    1\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Split Training and Testing data\n",
        "# X is the input data and y is the target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_in, y_out, test_size=0.2, random_state = 42)\n",
        "\n",
        "print(\"X_train size:\", X_train.shape)\n",
        "print(\"y_train size:\", y_train.shape)\n",
        "print(\"X_test size:\", X_test.shape)\n",
        "print(\"y_test size:\", y_test.shape) \n",
        "\n",
        "print(X_train.head())\n",
        "print(X_test.head())\n",
        "print(y_train.head())\n",
        "print(y_test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "id": "f66fa5e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f66fa5e9",
        "outputId": "cd42142a-5aed-49ad-beb1-f9feaf4e2a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.float64\n",
            "torch.int64\n",
            "torch.int64\n",
            "tensor([[0.4118, 0.6482, 0.5574, 0.4949, 0.1478, 0.5738, 0.1541],\n",
            "        [0.2941, 0.7940, 0.5738, 0.0000, 0.0000, 0.4441, 0.0551],\n",
            "        [0.0000, 0.6935, 0.0000, 0.0000, 0.0000, 0.5410, 0.3651],\n",
            "        [0.0588, 0.4020, 0.4508, 0.0000, 0.0000, 0.2846, 0.0769]],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# convert DF to PyTorch tensor\n",
        "X_train_tensor = torch.tensor(X_train.values)\n",
        "X_test_tensor = torch.tensor(X_test.values)\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train.values)\n",
        "y_test_tensor = torch.tensor(y_test.values)\n",
        "\n",
        "print(X_train_tensor.dtype)\n",
        "print(X_test_tensor.dtype)\n",
        "print(y_train_tensor.dtype)\n",
        "print(y_test_tensor.dtype)\n",
        "\n",
        "print(X_train_tensor[0:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "id": "FTwiPyabg0AQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTwiPyabg0AQ",
        "outputId": "bb0ef97b-3f01-4be7-c9e2-ebad1117d352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.float64\n"
          ]
        }
      ],
      "source": [
        "# making tensors\n",
        "# convert dtype for y (to prevent the error): RuntimeError: mat1 and mat2 must have the same dtype\n",
        "y_train_tensor = y_train_tensor.to(dtype=torch.float64)\n",
        "y_test_tensor = y_test_tensor.to(dtype=torch.float64)\n",
        "\n",
        "print(y_train_tensor.dtype)\n",
        "print(y_test_tensor.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "id": "LFOqdGxLagsG",
      "metadata": {
        "id": "LFOqdGxLagsG"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_i = self.X[idx]\n",
        "        y_i = self.y[idx]\n",
        "        return x_i, y_i  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "id": "0M8RjwotbAHE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M8RjwotbAHE",
        "outputId": "c604b212-33fc-4590-f10f-f5f5c787d72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n",
            "torch.float64\n",
            "torch.float64\n",
            "torch.float64\n",
            "tensor([[0.4118, 0.6482, 0.5574, 0.4949, 0.1478, 0.5738, 0.1541],\n",
            "        [0.2941, 0.7940, 0.5738, 0.0000, 0.0000, 0.4441, 0.0551],\n",
            "        [0.0000, 0.6935, 0.0000, 0.0000, 0.0000, 0.5410, 0.3651],\n",
            "        [0.0588, 0.4020, 0.4508, 0.0000, 0.0000, 0.2846, 0.0769]],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# Create a dataset instance\n",
        "train_dataset = MyDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = MyDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "print(X_train_tensor.dtype)\n",
        "print(y_train_tensor.dtype)\n",
        "print(X_test_tensor.dtype)\n",
        "print(y_test_tensor.dtype)\n",
        "\n",
        "#print(X_train_tensor[(0,1), :])\n",
        "print(X_train_tensor[0:4])\n",
        "#print(X_train_tensor[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "id": "b4aff297",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4aff297",
        "outputId": "fc676c2d-686a-48e8-81e4-01d8bc3a8c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set has 38 instances\n",
            "Validation set has 152 instances\n"
          ]
        }
      ],
      "source": [
        "# Create data loaders for datasets \n",
        "# shuffle for training, not for validation\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle=True) # 32, 64, 38\n",
        "#test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 38, shuffle=False) # 8, 16, 38\n",
        "\n",
        "print('Training set has {} instances'.format(len(train_dataloader)))\n",
        "print('Validation set has {} instances'.format(len(test_dataset)))\n",
        "\n",
        "#for idx, (X, y) in enumerate(test_dataset):\n",
        "#  print(idx, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "id": "41329171",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41329171",
        "outputId": "499fba77-2b4f-4756-ca21-8112a9c43b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (7): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 323
        }
      ],
      "source": [
        "# class NeuralNetwork(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(NeuralNetwork, self).__init__()\n",
        "#         self.linear_relu_stack = nn.Sequential(\n",
        "#             nn.Linear(7, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(128, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(128, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         logits = self.linear_relu_stack(x)\n",
        "#         return logits\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = NeuralNetwork().to(device)\n",
        "# print(model)\n",
        "# device\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "id": "6d6e110a",
      "metadata": {
        "id": "6d6e110a"
      },
      "outputs": [],
      "source": [
        "# define training and testing loops\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 4 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "\n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "id": "77b29689",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77b29689",
        "outputId": "6b78dea3-e9fa-4ab4-b881-3275578b2591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train loss: 0.431695 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.581153 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.430882 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.398608 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.439457 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.477137 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.444101 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.478779 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.460706 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.766126 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0310 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5264 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train loss: 0.561342 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.498703 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.445998 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.481463 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.629036 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.410854 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.378314 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.411845 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.697632 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.712081 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0309 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5222 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train loss: 0.500440 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.534465 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.449873 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.601413 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.562942 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.397436 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.463238 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.451286 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.581724 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.478913 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0307 \n",
            "\n",
            "Test Accuracy: 118/152 (77.63%), Avg Test loss: 0.5252 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train loss: 0.351216 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.504546 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.594269 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.353886 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.505684 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.413283 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.836951 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.520338 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.488964 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.409356 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0306 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5192 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train loss: 0.537448 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.548396 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.414661 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.434690 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.308641 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.520964 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.314096 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.349700 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.626301 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.564887 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 78/608 (12.83%), Avg Train loss: 0.0304 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5175 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train loss: 0.513746 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.417765 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.419152 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.584185 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.531373 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.374965 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.587365 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.543745 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.521952 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.797521 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0302 \n",
            "\n",
            "Test Accuracy: 118/152 (77.63%), Avg Test loss: 0.5205 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train loss: 0.499609 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.666446 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.596907 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.499335 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.424079 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.260398 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.581591 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.519682 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.471786 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.390941 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0302 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5159 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train loss: 0.388809 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.528683 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.432692 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.467216 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.593585 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.499697 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.537909 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.293082 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.504073 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.534783 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 78/608 (12.83%), Avg Train loss: 0.0302 \n",
            "\n",
            "Test Accuracy: 116/152 (76.32%), Avg Test loss: 0.5171 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train loss: 0.485830 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.596983 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.811206 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.390872 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.617346 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.548224 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.366476 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.429560 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.604619 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.372547 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0299 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5138 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train loss: 0.542170 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.464066 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.369220 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.615434 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.365141 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.608974 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.389921 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.846049 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.446785 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.601107 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0299 \n",
            "\n",
            "Test Accuracy: 117/152 (76.97%), Avg Test loss: 0.5167 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Train loss: 0.295114 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.310769 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.480727 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.481872 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.337135 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.329146 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.563801 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.230568 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.515655 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.541221 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0298 \n",
            "\n",
            "Test Accuracy: 117/152 (76.97%), Avg Test loss: 0.5164 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Train loss: 0.297109 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.615276 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.432395 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.500730 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.494517 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.423629 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.423829 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.397304 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.370744 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.532627 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0294 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5133 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Train loss: 0.641603 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.556445 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.588110 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.447234 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.503741 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.446704 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.402928 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.594829 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.315847 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.374236 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0296 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5129 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Train loss: 0.601067 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.886177 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.645217 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.361536 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.426597 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.506307 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.429081 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.485256 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.393763 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.480470 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0297 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5120 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Train loss: 0.294368 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.491383 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.475848 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.295218 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.494090 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.268514 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.538574 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.586029 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.521927 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.663704 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0296 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5117 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Train loss: 0.762620 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.484368 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.442730 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.328407 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.429287 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.481894 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.459655 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.286927 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.488604 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.416600 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0295 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5116 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Train loss: 0.354075 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.570802 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.383019 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.514426 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.536986 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.513769 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.367223 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.265429 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.446928 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.366885 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0293 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5171 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Train loss: 0.329790 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.506995 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.602661 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.324183 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.431450 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.335417 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.579152 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.549636 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.408181 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.326587 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0294 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5154 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Train loss: 0.643924 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.419059 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.283674 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.506231 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.634754 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.461288 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.418358 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.274562 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.596672 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.425410 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0294 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5159 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Train loss: 0.358635 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.377648 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.518491 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.404932 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.513105 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.519228 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.380463 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.512202 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.434998 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.395875 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 76/608 (12.50%), Avg Train loss: 0.0294 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5107 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Train loss: 0.336012 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.478275 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.469397 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.414712 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.509685 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.579135 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.456770 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.355465 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.409538 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.546017 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 80/608 (13.16%), Avg Train loss: 0.0293 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5100 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Train loss: 0.649352 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.331179 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.269464 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.266354 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.469862 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.574446 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.537296 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.439953 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.514850 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.553843 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0293 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5096 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Train loss: 0.548627 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.476246 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.388331 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.471512 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.474749 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.472520 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.579421 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.328754 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.436120 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.782366 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0292 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5121 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Train loss: 0.415444 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.410487 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.414710 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.376311 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.196796 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.479343 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.427011 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.496234 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.621808 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.436027 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0293 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5095 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Train loss: 0.276853 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.682934 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.501727 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.412154 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.423904 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.282995 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.805014 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.370053 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.342372 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.635601 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0292 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5093 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Train loss: 0.456049 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.446601 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.345861 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.485506 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.493520 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.298194 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.520447 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.596803 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.440187 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.339466 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0293 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5097 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Train loss: 0.307759 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.467669 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.445513 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.464870 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.371153 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.804887 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.423719 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.274637 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.565457 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.424062 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 80/608 (13.16%), Avg Train loss: 0.0292 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5094 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Train loss: 0.395197 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.400759 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.290616 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.303101 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.387901 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.435485 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.427710 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.421730 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.591028 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.503708 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0291 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5135 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Train loss: 0.516757 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.397440 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.425321 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.670115 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.672203 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.418356 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.367438 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.314191 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.256809 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.406428 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 78/608 (12.83%), Avg Train loss: 0.0292 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5107 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Train loss: 0.452622 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.417063 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.639062 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.349256 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.620911 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.265509 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.587759 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.389590 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.268160 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.363670 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 101/608 (16.61%), Avg Train loss: 0.0291 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5091 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Train loss: 0.331222 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.437489 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.456781 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.428027 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.459191 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.762854 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.513380 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.456157 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.406254 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.445260 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0291 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5093 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Train loss: 0.472844 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.518490 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.533408 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.569687 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.328498 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.361713 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.324748 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.464935 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.485678 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.245540 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0291 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5089 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Train loss: 0.497207 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.695449 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.419943 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.534305 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.506756 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.537225 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.240547 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.307807 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.323225 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.379867 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5093 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Train loss: 0.305611 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.296433 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.575617 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.532772 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.545891 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.704909 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.561553 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.472685 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.398119 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.182135 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0291 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5094 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Train loss: 0.687780 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.390700 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.556049 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.422002 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.243792 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.698780 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.339378 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.371070 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.618196 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.634252 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0291 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5088 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Train loss: 0.586883 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.605904 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.439526 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.510463 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.547722 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.418681 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.399213 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.219000 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.658552 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.547656 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5200 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Train loss: 0.357482 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.582302 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.738870 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.463430 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.417081 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.443300 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.483385 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.578794 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.330413 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.302348 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5087 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Train loss: 0.324320 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.412007 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.507498 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.392557 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.253139 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.285690 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.420291 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.345621 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.643795 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.779076 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5188 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Train loss: 0.392519 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.368281 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.294801 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.444054 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.434147 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.425582 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.470396 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.377322 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.546387 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.420446 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5137 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Train loss: 0.492164 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.680502 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.370335 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.756105 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.376520 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.425130 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.490305 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.184330 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.495140 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.446282 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5092 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Train loss: 0.430044 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.317228 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.475091 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.694301 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.597189 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.424291 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.270754 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.395843 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.354072 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.499533 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5105 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Train loss: 0.356211 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.421092 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.320811 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.624023 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.423025 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.780504 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.413281 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.206770 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.558256 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.480165 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 77/608 (12.66%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5093 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Train loss: 0.425811 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.500358 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.564358 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.292408 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.534898 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.432950 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.390025 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.333106 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.524357 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.612969 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5129 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Train loss: 0.418581 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.586324 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.377841 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.610206 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.436891 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.289243 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.514122 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.503405 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.444077 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.444552 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5110 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Train loss: 0.444389 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.387031 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.460087 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.466743 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.546168 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.432519 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.642260 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.592090 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.226073 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.410795 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5115 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Train loss: 0.495823 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.329719 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.323773 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.379199 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.511229 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.331698 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.479617 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.484026 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.282582 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.644048 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5130 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Train loss: 0.393136 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.246250 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.494265 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.513967 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.332381 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.404188 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.341622 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.820240 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.714041 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.495178 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5122 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Train loss: 0.391145 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.278142 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.303914 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.559337 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.389242 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.525186 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.410345 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.331642 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.297096 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.660585 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5094 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Train loss: 0.603238 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.422692 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.299449 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.457122 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.531190 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.494215 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.659952 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.323233 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.388079 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.647592 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5094 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Train loss: 0.446766 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.497480 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.589645 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.327874 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.602805 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.531181 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.445499 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.425403 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.491708 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.299560 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5098 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Train loss: 0.382799 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.650906 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.554342 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.768852 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.402890 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.523436 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.355987 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.730167 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.465645 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.351081 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5118 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Train loss: 0.405966 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.407880 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.424321 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.395290 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.574536 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.385179 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.456148 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.300876 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.214385 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.391384 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5091 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Train loss: 0.581316 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.394970 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.473542 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.383772 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.485681 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.313259 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.512136 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.359315 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.374974 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.299597 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 78/608 (12.83%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5095 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Train loss: 0.390061 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.516452 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.736651 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.373370 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.320656 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.621660 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.556969 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.384792 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.315191 in Batch: 32  [  528/  608]\n",
            "Train loss: 1.070855 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0291 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5099 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Train loss: 0.551463 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.629777 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.350766 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.375657 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.412270 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.517385 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.612181 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.329769 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.438077 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.285538 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5100 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Train loss: 0.380713 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.418296 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.492632 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.304337 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.375738 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.572545 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.431008 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.484292 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.747326 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.465589 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 76/608 (12.50%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5101 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Train loss: 0.463296 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.268762 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.506040 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.517886 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.603960 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.522993 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.434873 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.512447 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.325066 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.331563 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 80/608 (13.16%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5096 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Train loss: 0.396220 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.488207 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.338022 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.394076 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.430024 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.811796 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.336373 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.464022 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.534697 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.523525 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5098 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Train loss: 0.320706 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.553798 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.480661 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.322888 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.274556 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.347926 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.434988 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.407138 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.783234 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.445518 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5118 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Train loss: 0.603884 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.555134 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.509052 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.471128 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.483778 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.462689 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.520042 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.569234 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.372167 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.239881 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5087 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Train loss: 0.310989 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.514686 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.526746 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.511698 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.331360 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.454125 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.562771 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.578410 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.591945 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.298137 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5128 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Train loss: 0.449537 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.365078 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.428406 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.590182 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.760473 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.345900 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.342346 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.275031 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.789136 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.385032 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5099 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Train loss: 0.571115 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.471105 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.603104 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.433711 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.633743 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.392415 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.206331 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.385096 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.496842 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.452603 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5106 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Train loss: 0.364207 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.482240 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.509347 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.467485 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.527147 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.694020 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.456293 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.353125 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.232046 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.667739 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5123 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Train loss: 0.334989 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.235444 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.440842 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.388541 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.307254 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.523519 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.764110 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.545381 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.580097 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.408023 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5148 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Train loss: 0.326103 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.327061 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.401119 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.534752 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.259719 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.456408 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.350322 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.354365 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.855654 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.601702 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5092 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Train loss: 0.498644 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.396769 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.321217 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.465571 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.391093 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.544383 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.411961 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.682317 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.853611 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.342586 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5093 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Train loss: 0.293163 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.328135 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.768730 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.325192 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.394614 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.632414 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.734221 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.422063 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.543973 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.448262 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5094 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Train loss: 0.433483 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.731294 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.420907 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.451285 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.256947 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.467839 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.391961 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.333420 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.744910 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.303437 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5170 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Train loss: 0.543578 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.782806 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.247076 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.461427 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.427718 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.323072 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.559331 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.438993 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.815370 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.387340 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5102 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Train loss: 0.557425 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.428960 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.379440 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.585640 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.242608 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.435182 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.502926 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.544644 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.458694 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.580262 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0290 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5170 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Train loss: 0.321068 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.610077 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.347141 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.407209 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.277463 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.363139 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.348837 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.520057 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.503894 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.466963 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5095 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Train loss: 0.357403 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.342952 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.523984 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.363931 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.427235 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.312418 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.520271 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.309894 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.662785 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.425172 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5188 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Train loss: 0.329654 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.385571 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.469561 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.620530 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.509878 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.592962 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.424578 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.211763 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.510602 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.487414 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5098 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Train loss: 0.464565 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.397997 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.556775 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.603568 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.501561 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.538412 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.499051 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.397495 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.531484 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.454568 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5104 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Train loss: 0.484936 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.337461 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.484401 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.423788 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.477549 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.237718 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.659311 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.354054 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.368478 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.511954 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5104 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Train loss: 0.378096 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.517007 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.438340 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.382877 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.479315 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.467713 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.369690 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.341724 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.476713 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.294398 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5105 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Train loss: 0.507297 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.486304 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.304048 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.615516 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.473352 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.555755 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.513235 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.164590 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.388491 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.452235 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5115 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Train loss: 0.401603 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.241875 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.415199 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.344245 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.534776 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.433098 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.655034 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.327251 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.388363 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.301469 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5113 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Train loss: 0.541053 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.490421 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.743387 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.285669 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.479045 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.500655 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.372062 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.395086 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.485428 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.381081 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5114 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Train loss: 0.372192 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.612967 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.320996 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.447400 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.358094 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.527854 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.525986 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.475560 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.619573 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.450039 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5099 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Train loss: 0.615887 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.458653 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.632280 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.282287 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.503188 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.377984 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.366904 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.475678 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.553818 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.691192 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5100 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Train loss: 0.537368 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.344051 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.392155 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.336328 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.565943 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.508318 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.630142 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.491192 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.354767 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.561453 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5137 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Train loss: 0.553205 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.404478 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.424424 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.749730 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.311803 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.616327 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.223091 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.468343 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.360695 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.353004 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5099 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Train loss: 0.512362 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.351024 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.541139 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.439006 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.397551 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.309580 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.579060 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.298846 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.646397 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.374872 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5100 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Train loss: 0.534359 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.501546 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.566215 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.381334 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.476946 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.338470 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.261415 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.539519 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.443341 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.497770 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5104 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Train loss: 0.298270 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.344008 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.566948 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.398500 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.198349 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.431025 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.565401 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.430341 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.515772 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.440489 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0293 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5100 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Train loss: 0.740100 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.426925 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.284178 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.440679 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.279272 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.441688 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.556148 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.455784 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.721524 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.387929 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0287 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5158 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Train loss: 0.318614 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.317614 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.439459 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.332857 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.599707 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.187413 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.419023 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.335487 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.388716 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.307330 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5108 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Train loss: 0.431784 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.542816 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.372964 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.800655 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.385594 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.425021 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.398027 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.451971 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.478617 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.310097 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5115 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Train loss: 0.388312 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.519795 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.296306 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.519895 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.346729 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.406208 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.529475 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.506718 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.509900 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.473786 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5134 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Train loss: 0.340477 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.684259 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.285822 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.546424 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.526328 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.486189 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.404108 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.359799 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.944716 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.310146 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5106 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Train loss: 0.461584 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.494668 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.309434 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.618836 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.619172 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.522234 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.991167 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.338970 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.743798 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.346695 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5101 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Train loss: 0.367766 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.733409 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.673147 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.351683 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.494749 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.288663 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.442730 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.423534 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.497419 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.472054 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5120 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Train loss: 0.405383 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.289694 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.392267 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.473349 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.620931 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.430594 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.398024 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.401069 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.638070 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.320598 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0289 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5117 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Train loss: 0.505521 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.384621 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.261207 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.556344 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.586928 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.414842 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.554476 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.456094 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.306758 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.447934 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5102 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Train loss: 0.251466 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.391295 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.340233 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.459628 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.508471 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.406179 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.315474 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.587060 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.446074 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.435887 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0287 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5100 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Train loss: 0.372155 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.657856 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.460019 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.561403 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.576539 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.399698 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.403619 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.384845 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.650329 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.686783 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 79/608 (12.99%), Avg Train loss: 0.0287 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5152 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Train loss: 0.448266 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.538609 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.630672 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.440270 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.611522 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.430326 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.521180 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.287378 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.492475 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.544881 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0287 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5114 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Train loss: 0.482067 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.363660 in Batch:  4  [   80/  608]\n",
            "Train loss: 0.641274 in Batch:  8  [  144/  608]\n",
            "Train loss: 0.595476 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.297657 in Batch: 16  [  272/  608]\n",
            "Train loss: 0.345082 in Batch: 20  [  336/  608]\n",
            "Train loss: 0.602023 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.798006 in Batch: 28  [  464/  608]\n",
            "Train loss: 0.521900 in Batch: 32  [  528/  608]\n",
            "Train loss: 0.359615 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0288 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5119 \n",
            "\n",
            "Done!\n",
            "Max accuracy: 77.63157894736842\n"
          ]
        }
      ],
      "source": [
        "# Run model for epochs\n",
        "\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "id": "CwNNMfbrwqGX",
      "metadata": {
        "id": "CwNNMfbrwqGX"
      },
      "outputs": [],
      "source": [
        "# save weights\n",
        "import h5py\n",
        "# Save the model weights to an HDF5 file\n",
        "with h5py.File('rishabhv_nihareek_assignment2_part1.h5', 'w') as f:\n",
        "    for name, param in model.named_parameters():\n",
        "        f.create_dataset(name, data=param.detach().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot Train loss\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Fhsh6Ao3cOqf",
        "outputId": "1f2ac2ee-00cb-4a4c-9a4b-f70e2d7a71bc"
      },
      "id": "Fhsh6Ao3cOqf",
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot Test Loss\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "NjGolwBrSOn9",
        "outputId": "b116e73e-fee4-4a59-c3a9-88fec1f24155"
      },
      "id": "NjGolwBrSOn9",
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot Training and Testing loss on the same plot\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ilPrTHk8cCxf",
        "outputId": "fce8b04b-b94a-4cd2-843d-98b9af88d776"
      },
      "id": "ilPrTHk8cCxf",
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy curve\n",
        "plt.plot(train_acc_history, label='Train accuracy')\n",
        "plt.plot(test_acc_history, label='Test accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "7Yoy0drZrOCP",
        "outputId": "49f438ec-ba30-4282-d490-6b1a4a18cb81"
      },
      "id": "7Yoy0drZrOCP",
      "execution_count": 331,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "id": "ce564420",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ce564420",
        "outputId": "4ed5edb2-b8be-488f-8c18-4626dfd343a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test_tensor, y_pred)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')           # Visualize the confusion matrix using a heatmap\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "id": "77c88bbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "77c88bbc",
        "outputId": "9280d7fe-7efd-4bfc-deb5-7a9aa6e5f273"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# precision-recall curve and average precision score\n",
        "precision, recall, thresholds = precision_recall_curve(y_test_tensor, y_pred)\n",
        "average_precision = average_precision_score(y_test_tensor, y_pred)\n",
        "\n",
        "# Plot\n",
        "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "id": "4468225b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "4468225b",
        "outputId": "8382e733-2e8c-49d8-bc0d-a770e0f1300f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Calculate ROC and area under curve (AUC)\n",
        "fpr, tpr, thresholds = roc_curve(y_test_tensor, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC\n",
        "plt.plot(fpr, tpr, 'b', label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2"
      ],
      "metadata": {
        "id": "AhTANUYnFhvV"
      },
      "id": "AhTANUYnFhvV"
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "id": "LxVjiQzvF-AU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxVjiQzvF-AU",
        "outputId": "cecb3fe5-a95a-4497-99a0-4ba679b71918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=64, bias=True)\n",
            "    (1): Dropout(p=0.5, inplace=False)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (7): Dropout(p=0.5, inplace=False)\n",
            "    (8): ReLU()\n",
            "    (9): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "## Implementing 'dropout' with probability 0.5\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 64),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "id": "0aF-yx6iGoI1",
      "metadata": {
        "id": "0aF-yx6iGoI1"
      },
      "outputs": [],
      "source": [
        "## dropout with probability 0.5\n",
        "# define training and testing loops\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "id": "z331iTVNG1L8",
      "metadata": {
        "id": "z331iTVNG1L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e444e538-5ee5-4b69-9675-cfefe129032b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train loss: 0.643782 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.602102 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.610000 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.525022 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.630273 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.614731 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.665739 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.554549 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.710880 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.546771 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.509763 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.575170 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.566318 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0393 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6279 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train loss: 0.596340 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.601583 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.576123 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.650266 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.704016 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.708317 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.536808 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.638172 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.662092 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.680961 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.705286 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.531677 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.561551 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0396 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.6216 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train loss: 0.526276 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.531171 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.610247 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.614773 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.638231 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.757462 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.735758 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.598735 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.690459 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.712186 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.632018 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.626832 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.679962 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0395 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6176 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train loss: 0.627314 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.595065 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.757727 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.626082 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.712736 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.584293 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.660912 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.656939 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.674173 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.576275 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.638512 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.530523 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.660366 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0394 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6272 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train loss: 0.693611 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.505478 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.631444 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.554047 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.553046 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.748083 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.601246 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.707128 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.727641 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.611556 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.530434 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.668529 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.732938 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0396 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6280 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train loss: 0.680982 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.473213 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.588668 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.703467 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.584421 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.726716 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.538159 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.621700 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.691690 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.492641 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.574955 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.724774 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.566032 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 102/608 (16.78%), Avg Train loss: 0.0392 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6258 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train loss: 0.714567 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.563671 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.802120 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.541042 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.701303 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.641452 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.700487 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.596440 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.732643 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.648887 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.620495 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.786653 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.542690 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0389 \n",
            "\n",
            "Test Accuracy: 99/152 (65.13%), Avg Test loss: 0.6315 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train loss: 0.705014 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.734062 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.693332 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.692363 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.514049 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.601692 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.640333 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.605051 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.614634 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.633404 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.561269 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.731435 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.507081 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0389 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6191 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train loss: 0.613786 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.517258 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.605537 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.645550 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.600850 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.587253 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.716437 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.531474 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.617832 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.606872 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.495641 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.570520 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.753025 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0393 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.6312 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train loss: 0.705366 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.559160 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.651217 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.483705 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.660141 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.533036 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.593340 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.526926 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.582440 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.774063 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.466099 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.692589 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.507646 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0390 \n",
            "\n",
            "Test Accuracy: 98/152 (64.47%), Avg Test loss: 0.6155 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Train loss: 0.723164 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.580466 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.467488 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.773660 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.585734 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.841712 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.623914 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.540437 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.669851 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.588047 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.565991 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.634170 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.736579 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0392 \n",
            "\n",
            "Test Accuracy: 97/152 (63.82%), Avg Test loss: 0.6312 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Train loss: 0.648609 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.655500 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.767598 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.634938 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.748572 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.650541 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.642034 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.558888 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.598279 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.504103 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.676279 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.576858 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.573402 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0388 \n",
            "\n",
            "Test Accuracy: 99/152 (65.13%), Avg Test loss: 0.6292 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Train loss: 0.688240 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.512486 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.565680 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.544106 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.530228 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.700467 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.545954 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.636657 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.784138 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.549715 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.638267 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.719030 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.644892 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0386 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6318 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Train loss: 0.662733 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.569455 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.576936 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.571268 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.687117 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.734563 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.721664 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.687881 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.610139 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.673823 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.541795 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.641312 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.548328 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0394 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.6177 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Train loss: 0.776523 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.620980 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.640774 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.593675 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.648760 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.660799 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.634373 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.588488 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.600625 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.646278 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.534532 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.703278 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.568772 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0386 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6324 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Train loss: 0.491065 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.609018 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.675476 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.677507 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.675447 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.696395 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.626792 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.557132 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.589203 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.755780 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.582313 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.611535 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.656755 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0387 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6399 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Train loss: 0.601050 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.637526 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.446274 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.604416 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.603240 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.609127 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.718478 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.522258 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.566348 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.743024 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.599507 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.732733 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.676725 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0384 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6301 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Train loss: 0.753084 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.632459 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.652985 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.670339 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.638537 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.611671 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.731150 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.642311 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.695033 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.523202 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.696706 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.613959 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.652636 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0391 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6226 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Train loss: 0.641058 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.707257 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.620394 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.705969 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.597850 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.642419 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.683055 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.696420 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.582857 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.645368 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.599599 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.589664 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.647816 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0388 \n",
            "\n",
            "Test Accuracy: 97/152 (63.82%), Avg Test loss: 0.6432 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Train loss: 0.640999 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.537351 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.622848 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.594120 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.615939 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.550776 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.606942 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.532035 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.659670 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.532339 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.633035 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.598577 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.634861 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 101/608 (16.61%), Avg Train loss: 0.0386 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6161 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Train loss: 0.639828 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.574996 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.621848 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.703860 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.480296 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.706085 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.674852 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.532913 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.569739 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.567846 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.694129 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.658069 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.572467 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0387 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6210 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Train loss: 0.677636 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.556985 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.567110 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.497735 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.581473 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.561831 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.651752 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.575094 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.453503 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.841682 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.583528 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.527449 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.697356 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0379 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5965 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Train loss: 0.563805 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.668957 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.661274 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.602450 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.737391 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.559192 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.529620 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.486104 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.643812 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.750649 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.597744 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.601246 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.544227 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0377 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.6134 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Train loss: 0.598401 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.579514 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.613599 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.674045 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.681086 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.555054 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.601067 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.539566 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.725047 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.763046 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.508082 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.573965 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.716809 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0382 \n",
            "\n",
            "Test Accuracy: 97/152 (63.82%), Avg Test loss: 0.6246 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Train loss: 0.620060 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.469795 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.552330 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.582569 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.697023 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.586446 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.607861 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.735091 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.575889 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.678740 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.690481 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.744693 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.590596 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0386 \n",
            "\n",
            "Test Accuracy: 99/152 (65.13%), Avg Test loss: 0.6354 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Train loss: 0.626491 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.668773 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.604178 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.653004 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.650211 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.602996 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.520678 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.676824 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.546108 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.617599 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.592919 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.553165 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.596491 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0383 \n",
            "\n",
            "Test Accuracy: 99/152 (65.13%), Avg Test loss: 0.6294 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Train loss: 0.522990 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.636769 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.659619 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.549113 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.586364 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.707145 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.609172 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.613213 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.555626 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.610007 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.700048 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.561574 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.544702 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0381 \n",
            "\n",
            "Test Accuracy: 97/152 (63.82%), Avg Test loss: 0.6264 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Train loss: 0.629645 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.583640 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.623490 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.531684 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.521043 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.578304 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.570196 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.550377 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.695441 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.486282 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.567708 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.730456 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.472270 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0381 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.6132 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Train loss: 0.568846 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.548039 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.669720 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.581974 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.740920 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.585975 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.545787 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.580206 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.522736 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.560215 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.588364 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.592342 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.618542 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0378 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.6183 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Train loss: 0.682742 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.954352 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.511042 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.640759 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.641842 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.642813 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.539995 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.577707 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.547104 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.512278 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.651117 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.505094 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.531009 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 102/608 (16.78%), Avg Train loss: 0.0384 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.5973 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Train loss: 0.625211 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.493430 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.725629 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.619313 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.651866 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.699589 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.590518 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.649442 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.618488 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.585842 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.512419 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.549981 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.727377 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0378 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.5990 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Train loss: 0.549133 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.678411 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.552845 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.616297 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.574732 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.652075 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.609472 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.582509 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.573375 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.635430 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.570454 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.677607 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.633071 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0382 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.6024 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Train loss: 0.682175 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.664859 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.628637 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.484578 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.736740 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.558220 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.573244 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.528186 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.589912 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.530587 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.635580 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.563698 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.688729 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0382 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.6025 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Train loss: 0.551031 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.633360 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.543120 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.561862 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.563029 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.556409 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.729610 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.601249 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.560487 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.563267 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.663948 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.674108 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.644041 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0377 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6180 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Train loss: 0.677853 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.731189 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.623495 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.660155 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.547200 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.648416 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.577268 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.629891 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.589511 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.699158 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.657045 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.640545 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.491879 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0375 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.6205 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Train loss: 0.585428 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.707395 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.511859 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.548219 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.670814 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.628302 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.683514 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.743367 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.575630 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.593599 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.623668 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.492390 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.666198 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 101/608 (16.61%), Avg Train loss: 0.0374 \n",
            "\n",
            "Test Accuracy: 98/152 (64.47%), Avg Test loss: 0.6279 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Train loss: 0.588953 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.630706 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.430661 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.532074 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.566524 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.700314 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.623297 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.553310 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.572915 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.514359 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.696345 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.616665 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.507111 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0379 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6101 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Train loss: 0.595323 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.668530 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.720970 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.646098 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.638062 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.644521 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.657433 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.546049 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.509173 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.484171 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.539646 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.576237 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.683946 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0372 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.6162 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Train loss: 0.537899 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.546807 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.537840 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.565131 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.641652 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.695760 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.555089 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.553067 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.541022 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.521761 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.586939 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.639662 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.776498 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0370 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.6079 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Train loss: 0.570469 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.772868 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.562034 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.576805 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.679363 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.486221 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.579995 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.567106 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.543275 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.665692 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.668016 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.623468 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.687528 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0377 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.5941 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Train loss: 0.610763 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.652892 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.509552 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.588380 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.463364 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.685762 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.693395 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.512823 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.515014 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.668330 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.743240 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.798966 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.668432 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0381 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.5955 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Train loss: 0.672791 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.543585 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.621360 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.432079 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.636324 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.634142 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.613020 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.636153 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.612201 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.574054 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.779416 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.633563 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.625726 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0374 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.6120 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Train loss: 0.651588 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.483740 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.740577 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.542445 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.577644 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.649796 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.633584 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.621632 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.660804 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.564988 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.729120 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.666653 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.783051 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0371 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6312 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Train loss: 0.792933 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.634574 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.655557 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.500259 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.485716 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.635336 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.547415 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.737090 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.636590 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.709621 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.627162 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.691258 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.548741 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 103/608 (16.94%), Avg Train loss: 0.0384 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6087 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Train loss: 0.460900 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.486061 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.567498 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.609063 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.714081 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.546819 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.647837 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.605928 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.607227 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.601008 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.560670 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.546170 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.497913 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0364 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.6144 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Train loss: 0.559908 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.562625 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.512207 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.562609 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.433981 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.550457 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.539524 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.588784 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.660655 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.692169 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.696131 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.661127 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.565292 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0370 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.6130 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Train loss: 0.638448 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.587083 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.545430 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.642599 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.541731 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.506415 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.531945 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.601883 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.694758 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.641765 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.531350 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.711206 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.620766 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0367 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5962 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Train loss: 0.650576 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.523736 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.578661 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.623453 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.758674 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.590659 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.534872 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.656805 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.589617 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.791659 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.612226 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.429083 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.620568 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0380 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.6158 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Train loss: 0.517241 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.406216 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.487097 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.698728 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.662801 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.536760 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.607848 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.659254 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.554513 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.615898 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.779832 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.606130 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.784574 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0378 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6105 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Train loss: 0.510834 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.665407 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.565420 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.636898 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.398644 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.701372 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.471559 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.596584 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.585445 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.616150 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.634408 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.501079 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.666528 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0374 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5923 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Train loss: 0.726619 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.623641 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.518347 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.671729 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.657635 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.525547 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.736839 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.504392 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.618612 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.614950 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.678284 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.579152 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.664035 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0374 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5980 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Train loss: 0.553566 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.575490 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.584456 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.802758 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.626312 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.643206 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.538341 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.493125 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.599348 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.628117 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.518371 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.465009 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.717257 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0360 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5848 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Train loss: 0.457952 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.631473 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.566557 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.521900 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.568696 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.414577 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.499062 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.546327 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.648283 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.683605 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.499780 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.525004 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.655117 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0368 \n",
            "\n",
            "Test Accuracy: 98/152 (64.47%), Avg Test loss: 0.5935 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Train loss: 0.671102 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.568909 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.704446 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.544172 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.639207 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.655470 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.557640 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.617607 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.500093 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.525988 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.552382 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.551989 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.701973 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0369 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5881 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Train loss: 0.667591 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.548840 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.602179 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.869539 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.528397 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.516071 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.698707 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.616715 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.617315 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.545046 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.476812 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.699299 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.657546 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0369 \n",
            "\n",
            "Test Accuracy: 96/152 (63.16%), Avg Test loss: 0.6392 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Train loss: 0.656312 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.614314 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.594052 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.434056 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.512625 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.520754 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.509622 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.498515 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.890176 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.556213 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.542423 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.469603 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.658599 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0366 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.6322 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Train loss: 0.413107 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.539187 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.664089 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.588109 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.588519 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.547218 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.455338 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.659263 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.527624 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.692701 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.528930 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.704430 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.567053 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0368 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.5949 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Train loss: 0.655705 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.570510 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.628234 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.599242 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.564604 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.490475 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.479138 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.526133 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.584448 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.543482 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.475564 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.742528 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.639982 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0360 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.6157 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Train loss: 0.513676 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.544099 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.614364 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.791107 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.414462 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.568605 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.703447 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.473120 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.558645 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.563995 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.567322 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.508371 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.561705 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0360 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.5980 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Train loss: 0.646189 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.452706 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.723900 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.445768 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.600290 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.629026 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.567308 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.537911 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.685535 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.730363 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.406194 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.501212 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.567854 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6097 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Train loss: 0.682532 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.740884 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.578332 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.722870 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.683372 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.491893 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.711167 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.660995 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.596999 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.449015 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.542167 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.422635 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.423974 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0365 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6155 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Train loss: 0.598119 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.634789 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.805909 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.580577 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.523804 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.540636 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.523436 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.613279 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.562571 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.600642 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.418068 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.590710 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.538672 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0361 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5841 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Train loss: 0.567316 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.764638 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.649465 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.456084 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.753633 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.767976 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.571224 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.756466 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.438655 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.617087 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.435579 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.571256 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.496754 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0364 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.5812 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Train loss: 0.557816 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.525682 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.483670 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.569776 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.706790 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.756455 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.775144 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.615301 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.599809 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.614304 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.532846 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.555949 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.582240 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0363 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5673 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Train loss: 0.481663 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.543266 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.428966 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.541412 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.562114 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.596732 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.586307 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.638565 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.577643 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.537201 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.631454 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.529279 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.449072 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0353 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6155 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Train loss: 0.670986 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.676390 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.605678 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.619481 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.641176 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.706071 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.538994 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.598598 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.851133 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.644411 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.603577 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.487311 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.590277 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0362 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5746 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Train loss: 0.543081 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.573519 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.660326 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.716445 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.441605 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.579414 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.562598 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.700329 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.527331 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.497609 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.606935 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.615966 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.540057 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 103/608 (16.94%), Avg Train loss: 0.0357 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5897 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Train loss: 0.595149 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.558771 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.522749 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.366885 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.688403 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.536356 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.509732 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.508728 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.658619 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.549469 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.510086 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.475688 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.597895 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5700 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Train loss: 0.590846 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.453766 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.522414 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.454453 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.473180 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.458766 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.531046 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.527095 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.604955 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.616198 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.687697 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.541017 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.717504 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5903 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Train loss: 0.760117 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.558825 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.486289 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.657576 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.620542 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.534546 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.552694 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.310993 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.491677 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.725982 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.533396 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.666531 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.552374 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.6080 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Train loss: 0.724017 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.433409 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.632138 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.515359 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.476311 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.451128 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.569875 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.576295 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.497772 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.560496 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.448040 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.485173 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.579929 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0359 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5687 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Train loss: 0.707408 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.707010 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.481180 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.815269 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.667070 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.602272 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.484161 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.697309 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.662692 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.544293 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.572837 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.640156 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.680590 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0365 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.5893 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Train loss: 0.702181 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.663320 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.465808 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.560188 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.560308 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.496307 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.426871 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.614771 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.460055 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.588102 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.458527 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.599941 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.434730 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0354 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.5782 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Train loss: 0.577978 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.687935 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.503886 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.553199 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.630987 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.451317 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.607951 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.658178 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.516155 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.509334 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.591422 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.475576 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.865419 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0363 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5672 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Train loss: 0.555078 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.516159 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.590470 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.555237 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.637690 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.531212 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.581387 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.515699 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.501773 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.690324 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.585600 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.507952 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.486056 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0360 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.5793 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Train loss: 0.424450 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.558882 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.651832 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.659698 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.575550 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.865939 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.670010 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.831791 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.554130 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.524699 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.660179 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.651386 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.530114 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0360 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5731 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Train loss: 0.581668 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.507870 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.659019 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.496221 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.450241 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.676204 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.573469 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.478471 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.667068 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.524902 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.440278 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.734536 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.419971 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0354 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.5957 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Train loss: 0.477006 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.507424 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.733515 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.451024 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.513306 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.477305 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.558481 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.524099 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.678424 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.680386 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.358124 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.528903 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.625702 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0351 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.5964 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Train loss: 0.664396 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.617118 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.565729 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.493381 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.634203 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.422753 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.547219 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.586798 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.486668 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.472435 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.351135 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.541876 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.658300 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0362 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.6116 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Train loss: 0.443080 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.664968 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.645012 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.547210 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.685421 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.563254 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.570808 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.621661 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.469013 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.653829 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.372379 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.517376 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.490517 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0353 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6053 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Train loss: 0.498102 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.861391 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.484813 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.554167 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.585920 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.537339 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.563200 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.656991 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.601710 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.604292 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.688912 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.475268 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.417535 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5644 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Train loss: 0.581967 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.468237 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.532820 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.625287 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.547510 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.499088 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.526561 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.730508 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.350433 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.576111 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.443778 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.621100 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.560876 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0344 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5822 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Train loss: 0.450895 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.623714 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.637954 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.607652 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.438242 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.457224 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.517774 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.620080 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.666210 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.577525 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.726118 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.582779 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.541632 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0348 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5802 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Train loss: 0.511854 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.562797 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.651681 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.463040 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.685229 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.578011 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.531235 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.621009 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.488133 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.519539 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.486163 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.586141 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.645900 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0347 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5746 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Train loss: 0.549359 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.593790 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.631176 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.652095 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.543658 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.695316 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.691811 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.479548 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.403289 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.433576 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.640581 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.566857 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.646279 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.5914 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Train loss: 0.444042 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.550336 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.539535 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.559707 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.476018 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.861581 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.504521 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.563539 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.463259 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.489349 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.490968 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.649465 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.458848 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5646 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Train loss: 0.563183 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.639052 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.616272 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.509478 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.705645 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.547314 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.425433 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.614515 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.667540 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.516480 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.421301 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.590545 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.586960 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0350 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.5869 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Train loss: 0.481612 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.606238 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.483964 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.464427 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.523716 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.644152 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.665738 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.693256 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.462483 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.437483 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.470713 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.544965 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.598477 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0356 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5783 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Train loss: 0.614793 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.498537 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.536500 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.491052 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.684114 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.581162 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.514511 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.387666 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.454363 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.681370 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.626587 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.571408 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.575592 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.5661 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Train loss: 0.657218 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.542039 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.612197 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.576889 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.434734 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.528677 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.660904 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.652269 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.493566 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.465039 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.797780 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.495289 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.595900 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0350 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5583 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Train loss: 0.683652 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.407669 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.521410 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.604919 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.473221 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.478302 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.715438 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.493334 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.499983 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.590042 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.420878 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.497277 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.465386 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0339 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5655 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Train loss: 0.683754 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.557889 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.581386 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.828738 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.637296 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.566680 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.575639 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.645674 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.728458 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.566905 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.546017 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.557321 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.566977 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0352 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5649 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Train loss: 0.698242 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.570613 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.518447 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.489861 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.587256 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.670676 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.708447 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.532127 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.796472 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.506241 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.599102 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.539273 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.660517 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0352 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5576 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Train loss: 0.564197 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.570716 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.554197 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.504023 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.617815 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.518006 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.483038 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.598592 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.495515 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.537015 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.693684 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.743406 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.588348 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0342 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5667 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Train loss: 0.492019 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.451926 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.728208 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.513759 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.494121 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.707390 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.549000 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.722144 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.512714 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.620194 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.524222 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.436737 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.527294 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0352 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5810 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Train loss: 0.396738 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.568022 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.527040 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.533558 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.672392 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.484930 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.631466 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.437333 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.581637 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.476445 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.480601 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.580487 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.450501 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0338 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.5943 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Train loss: 0.837532 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.460457 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.614698 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.505327 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.604835 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.408849 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.507971 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.704840 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.588757 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.584779 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.686960 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.301476 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.670426 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0356 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.5738 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Train loss: 0.532683 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.691708 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.529708 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.546598 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.580989 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.592839 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.728219 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.501255 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.506843 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.488443 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.458139 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.550695 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.663824 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0346 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5791 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Train loss: 0.681372 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.438617 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.390001 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.521780 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.564169 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.647435 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.650174 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.428069 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.470026 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.629324 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.587415 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.511128 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.683713 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0331 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.5686 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Train loss: 0.573118 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.489659 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.536015 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.417877 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.709763 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.511455 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.502977 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.706258 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.365875 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.518405 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.355346 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.578736 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.628173 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0346 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5326 \n",
            "\n",
            "Done!\n",
            "Max accuracy: 75.0\n"
          ]
        }
      ],
      "source": [
        "## dropout with probability 0.5\n",
        "# Run model for epochs\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "id": "QkesK4cSNNeR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkesK4cSNNeR",
        "outputId": "13b83a26-6281-4e91-83c9-ef8bcdf07673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=64, bias=True)\n",
            "    (1): Dropout(p=0.25, inplace=False)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
            "    (4): Dropout(p=0.25, inplace=False)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (7): Dropout(p=0.25, inplace=False)\n",
            "    (8): ReLU()\n",
            "    (9): Linear(in_features=64, out_features=1, bias=True)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 340
        }
      ],
      "source": [
        "## dropout with probability 0.25\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 64),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "id": "dVc9FVwENlPP",
      "metadata": {
        "id": "dVc9FVwENlPP"
      },
      "outputs": [],
      "source": [
        "## dropout with probability 0.25\n",
        "# define training and testing loops\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "id": "x_sFDIfbN8Gk",
      "metadata": {
        "id": "x_sFDIfbN8Gk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30833963-852d-4f49-af7f-ea55908930af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train loss: 0.694065 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.688527 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.680518 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.688494 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.687207 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.687178 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.686395 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.697466 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.689143 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.688456 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.702437 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.685795 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.674239 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0429 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6816 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train loss: 0.686331 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.673562 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.699448 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.668833 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.661473 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.677965 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.670219 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.687132 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.674553 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.696419 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.683976 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.650421 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.684989 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0424 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6732 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train loss: 0.638168 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.643722 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.651871 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.638754 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.663183 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.676552 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.651653 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.663996 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.663659 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.712967 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.678833 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.696631 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.645222 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0419 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6696 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train loss: 0.643867 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.698641 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.661964 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.676521 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.657550 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.695540 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.644268 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.638825 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.654443 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.681381 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.676711 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.617707 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.626130 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0417 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6625 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train loss: 0.669850 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.634814 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.683762 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.642051 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.657624 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.652248 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.711865 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.693355 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.696247 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.678361 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.714216 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.667971 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.646024 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0414 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6586 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train loss: 0.608624 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.688313 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.678521 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.733434 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.585948 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.620780 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.646969 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.650322 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.647345 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.650295 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.650632 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.623796 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.648546 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0412 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6547 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train loss: 0.715952 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.702738 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.625766 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.683181 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.667757 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.665896 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.657534 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.642499 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.670916 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.565196 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.687742 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.597589 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.686445 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0411 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6551 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train loss: 0.614522 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.619012 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.639166 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.756128 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.662060 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.635124 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.605557 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.613497 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.684307 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.617355 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.662956 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.638047 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.690796 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0409 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6529 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train loss: 0.689691 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.589585 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.712879 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.722666 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.670880 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.633398 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.612182 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.584410 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.587283 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.559368 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.638140 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.692420 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.694914 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0409 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6503 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train loss: 0.641216 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.636796 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.691551 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.660943 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.636378 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.604835 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.596245 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.644691 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.686877 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.631894 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.664976 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.587751 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.723606 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0408 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6516 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Train loss: 0.662587 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.688476 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.640889 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.627208 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.607452 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.631943 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.732457 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.665929 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.665861 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.599627 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.602801 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.550837 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.684077 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0407 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6487 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Train loss: 0.697845 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.637076 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.660885 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.690282 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.608029 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.697338 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.607296 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.637522 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.633456 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.579843 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.640980 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.655295 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.534655 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0407 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6462 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Train loss: 0.658310 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.631342 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.619685 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.664743 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.634382 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.626372 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.670917 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.729546 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.563372 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.663338 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.687884 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.604047 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.733713 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0408 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6491 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Train loss: 0.730714 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.664092 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.598776 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.604827 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.659942 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.623744 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.591331 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.471914 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.661760 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.665961 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.686734 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.843675 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.607451 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0407 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6462 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Train loss: 0.603205 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.562548 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.662012 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.666411 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.588234 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.696806 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.631798 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.467257 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.554441 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.598635 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.650067 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.664079 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.606064 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6461 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Train loss: 0.650442 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.673242 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.661072 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.628710 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.696042 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.595833 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.688969 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.801674 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.567393 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.606932 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.626430 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.692857 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.672350 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6429 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Train loss: 0.586097 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.629912 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.675964 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.634676 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.676562 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.725593 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.661459 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.587900 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.592779 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.523792 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.763143 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.667745 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.631933 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0408 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6452 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Train loss: 0.598386 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.652439 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.588441 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.591800 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.559929 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.522731 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.737737 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.588058 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.636382 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.738066 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.671015 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.658650 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.624431 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6449 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Train loss: 0.633368 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.728283 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.589845 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.694220 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.693063 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.657557 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.699575 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.634641 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.695975 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.656376 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.624870 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.620337 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.704376 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6468 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Train loss: 0.665204 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.696466 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.585717 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.631616 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.658921 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.629439 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.659745 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.623289 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.546502 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.667861 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.690319 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.766578 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.629121 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6472 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Train loss: 0.666811 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.670002 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.557901 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.687851 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.594232 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.629932 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.564618 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.652970 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.736624 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.733076 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.730045 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.624428 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.590625 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0407 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6466 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Train loss: 0.582864 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.624247 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.726581 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.518494 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.636192 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.629633 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.672221 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.771203 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.595369 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.600665 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.485955 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.661280 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.621117 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6447 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Train loss: 0.583748 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.565670 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.697167 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.699330 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.672601 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.725056 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.591941 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.808710 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.633353 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.627129 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.701976 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.659648 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.734243 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0407 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6429 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Train loss: 0.626786 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.699388 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.706293 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.626538 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.658582 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.624482 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.594902 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.733221 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.631669 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.595418 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.478347 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.679187 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.628415 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0407 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6436 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Train loss: 0.737878 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.598474 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.588773 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.661514 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.621523 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.702820 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.599337 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.551859 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.733778 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.658064 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.626782 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.810876 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.632370 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6412 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Train loss: 0.599114 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.584807 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.711100 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.618357 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.638236 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.731100 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.721933 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.587710 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.594127 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.765761 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.584113 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.650751 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.591587 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6425 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Train loss: 0.618965 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.665585 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.800216 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.672912 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.627601 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.745624 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.581117 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.703533 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.659609 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.591384 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.690959 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.548951 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.552809 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6438 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Train loss: 0.700407 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.665192 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.660579 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.672700 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.627183 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.707786 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.660768 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.666475 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.583692 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.730397 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.553677 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.557668 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.633847 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6421 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Train loss: 0.690212 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.552477 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.668853 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.555193 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.590235 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.732534 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.737392 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.591998 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.587972 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.705836 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.809667 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.726871 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.516853 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6439 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Train loss: 0.554387 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.696544 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.699537 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.590782 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.665493 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.665833 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.585409 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.587577 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.702199 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.559431 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.655844 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.632251 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.634703 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0406 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6443 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Train loss: 0.631670 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.670995 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.725704 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.662757 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.633307 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.619143 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.694809 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.734233 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.622591 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.763408 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.562413 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.598105 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.551961 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6438 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Train loss: 0.623329 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.558181 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.594876 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.658907 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.703073 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.698657 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.813063 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.808103 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.702007 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.630965 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.592052 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.594237 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.592489 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6418 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Train loss: 0.647693 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.704972 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.709040 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.624517 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.656806 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.848754 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.663882 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.626987 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.626546 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.659710 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.593778 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.650339 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.738858 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6432 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Train loss: 0.623941 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.692834 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.664098 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.737482 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.627674 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.589372 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.549451 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.625951 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.813941 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.694289 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.551188 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.664417 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.619555 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6419 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Train loss: 0.777025 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.699438 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.728999 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.691704 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.555241 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.623040 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.624703 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.557578 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.667582 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.620061 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.627517 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.658876 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.672389 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6409 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Train loss: 0.582810 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.658305 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.596160 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.544826 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.668653 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.766830 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.699969 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.616664 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.696611 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.689851 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.591360 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.588645 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.589188 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6453 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Train loss: 0.694726 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.699664 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.656972 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.662080 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.593317 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.620489 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.773969 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.545840 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.699109 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.689083 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.588829 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.548209 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.658228 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6433 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Train loss: 0.697541 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.574737 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.765273 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.777792 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.627730 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.623909 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.552041 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.653758 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.691822 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.771359 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.701338 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.655089 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.511684 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6434 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Train loss: 0.668187 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.591897 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.599998 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.723144 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.661711 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.614900 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.765214 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.556948 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.588993 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.655584 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.630768 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.738978 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.630125 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0405 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6454 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Train loss: 0.661031 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.553412 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.544732 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.552118 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.588894 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.587606 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.696093 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.657313 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.667186 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.622774 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.707266 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.616248 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.652737 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6412 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Train loss: 0.763421 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.622697 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.699598 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.588096 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.656602 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.620992 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.623213 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.693197 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.616674 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.618062 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.589467 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.624969 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.619214 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6424 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Train loss: 0.651564 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.584567 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.540859 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.742198 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.662565 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.661762 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.628803 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.666775 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.592314 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.585834 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.626048 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.584034 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.705906 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6412 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Train loss: 0.546116 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.744440 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.660051 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.698067 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.731170 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.738338 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.729027 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.578409 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.548703 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.546483 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.658912 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.659280 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.511702 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6403 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Train loss: 0.631357 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.772084 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.656714 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.658805 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.662613 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.592339 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.656633 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.768877 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.658720 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.727484 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.510234 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.509630 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.619134 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6428 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Train loss: 0.659238 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.553701 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.693168 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.649596 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.659118 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.627980 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.644875 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.628515 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.619604 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.620046 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.807295 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.652941 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.554409 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6404 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Train loss: 0.845676 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.553817 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.660652 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.698454 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.700970 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.622498 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.582059 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.622419 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.691396 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.554572 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.616060 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.550082 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.546208 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6396 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Train loss: 0.586109 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.693219 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.732079 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.623704 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.659163 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.587707 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.617334 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.592343 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.551908 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.550373 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.590538 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.591010 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.691062 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6387 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Train loss: 0.708906 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.616384 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.544402 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.551258 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.579815 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.578983 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.585385 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.827762 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.633065 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.627195 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.510991 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.767510 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.721540 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6436 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Train loss: 0.472061 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.739355 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.584653 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.701892 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.737968 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.731120 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.767000 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.734290 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.662100 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.629113 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.619167 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.580988 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.729987 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6419 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Train loss: 0.618146 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.550384 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.765269 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.622407 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.503258 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.669906 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.728585 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.780418 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.801786 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.616349 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.578686 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.619998 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.699986 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6385 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Train loss: 0.592022 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.696121 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.654578 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.758889 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.810408 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.741761 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.647111 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.732831 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.690866 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.660157 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.616533 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.733799 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.512268 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6420 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Train loss: 0.618065 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.613662 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.585822 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.735495 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.840379 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.623873 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.548672 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.571997 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.697125 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.670117 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.776626 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.771895 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.788788 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6410 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Train loss: 0.659196 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.690688 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.737007 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.660264 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.633108 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.728120 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.789980 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.732481 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.634419 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.515982 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.560346 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.617153 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.729014 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0404 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6429 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Train loss: 0.671566 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.580173 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.647322 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.755106 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.775041 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.550371 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.696114 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.694328 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.644196 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.658792 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.573942 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.477494 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.621996 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6377 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Train loss: 0.655167 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.587484 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.617771 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.649110 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.607649 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.589328 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.549140 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.585867 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.554341 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.618241 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.612761 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.508974 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.732304 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6410 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Train loss: 0.582176 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.549186 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.620470 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.695607 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.545813 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.680654 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.553359 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.616576 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.796868 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.738448 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.544309 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.622807 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.738854 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6408 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Train loss: 0.580258 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.578251 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.763560 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.623051 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.657656 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.657029 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.582835 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.627232 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.618542 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.637133 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.592982 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.620519 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.730695 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6385 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Train loss: 0.777739 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.550740 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.535624 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.700659 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.687927 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.698448 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.656442 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.771735 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.581804 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.625189 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.648523 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.651511 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.587178 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0402 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6376 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Train loss: 0.690032 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.658761 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.612148 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.545713 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.724558 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.686283 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.691504 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.742151 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.658662 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.651555 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.487930 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.605461 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.653566 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6411 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Train loss: 0.577316 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.624783 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.550244 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.546373 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.690972 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.655103 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.546405 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.661449 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.656520 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.624162 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.686469 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.546140 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.623634 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0401 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6397 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Train loss: 0.860453 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.699371 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.662118 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.617240 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.519834 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.662322 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.621861 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.550269 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.714950 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.583494 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.572501 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.622989 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.653850 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0401 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6382 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Train loss: 0.726349 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.687253 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.549021 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.652229 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.515919 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.603443 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.573877 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.616258 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.543732 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.689194 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.579379 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.658696 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.612974 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6404 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Train loss: 0.669388 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.684154 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.662645 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.708450 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.650654 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.613425 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.698437 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.583781 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.729336 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.653721 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.504485 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.649208 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.617853 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0402 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6392 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Train loss: 0.544586 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.675953 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.618464 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.625245 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.690367 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.507095 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.772232 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.730029 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.582339 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.588409 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.583931 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.622278 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.657563 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0403 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6354 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Train loss: 0.607775 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.626332 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.686426 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.536455 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.837797 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.735071 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.573227 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.652447 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.700584 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.659851 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.602012 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.688990 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.556170 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0402 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6372 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Train loss: 0.548575 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.612268 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.678320 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.575722 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.735443 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.579285 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.581332 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.711562 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.616327 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.648782 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.543107 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.765837 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.721735 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0401 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6381 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Train loss: 0.680705 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.610770 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.687132 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.703619 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.497981 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.589411 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.613093 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.572262 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.584752 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.610801 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.609928 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.528784 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.690965 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6367 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Train loss: 0.555420 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.753596 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.652135 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.617029 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.717337 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.640861 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.616675 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.643870 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.697636 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.619169 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.616534 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.622343 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.739985 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6357 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Train loss: 0.629281 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.537455 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.625631 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.665601 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.791585 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.714993 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.582495 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.665243 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.619098 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.651105 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.571794 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.663852 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.562209 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6409 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Train loss: 0.797515 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.501112 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.501838 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.492762 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.608745 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.775605 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.642753 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.661871 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.577521 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.615970 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.729202 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.697228 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.569320 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6367 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Train loss: 0.643443 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.655257 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.491847 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.632404 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.617103 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.660144 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.655192 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.582276 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.706940 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.721268 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.537956 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.613633 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.623666 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0401 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6368 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Train loss: 0.754522 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.745505 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.645776 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.669660 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.656584 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.594074 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.605394 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.614047 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.582841 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.508017 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.682968 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.579309 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.661360 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0399 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6405 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Train loss: 0.590896 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.664147 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.529519 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.695879 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.636274 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.642902 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.620138 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.636619 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.423101 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.599956 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.576003 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.602728 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.572974 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0398 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6350 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Train loss: 0.653812 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.647160 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.605821 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.788898 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.562594 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.689019 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.502812 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.561181 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.721328 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.689362 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.573731 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.619848 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.625576 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6357 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Train loss: 0.577205 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.618653 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.605931 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.537428 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.608787 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.689262 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.660051 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.583283 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.647926 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.652139 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.602185 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.684854 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.655692 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6319 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Train loss: 0.626168 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.691165 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.577995 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.804328 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.704410 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.645438 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.656905 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.520487 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.695572 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.666750 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.650186 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.503062 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.557943 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0399 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6398 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Train loss: 0.759974 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.581065 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.712197 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.550053 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.665999 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.712019 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.680685 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.668233 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.532732 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.662107 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.666062 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.623823 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.560755 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0399 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6345 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Train loss: 0.660905 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.577615 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.647751 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.652167 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.700993 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.571465 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.627545 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.621500 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.573838 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.777448 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.756918 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.613711 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.733278 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0400 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6316 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Train loss: 0.755910 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.610726 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.664339 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.708086 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.623440 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.587565 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.496384 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.682351 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.689196 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.640387 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.719523 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.682690 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.630557 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0398 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6319 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Train loss: 0.610529 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.497398 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.577685 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.627152 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.542294 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.763403 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.650136 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.572585 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.660816 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.630395 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.573651 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.561160 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.583998 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0396 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6362 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Train loss: 0.720419 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.640198 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.597657 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.707811 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.711699 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.584276 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.548100 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.708148 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.694023 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.545721 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.612525 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.564485 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.604225 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0395 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6351 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Train loss: 0.730484 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.595472 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.491260 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.577849 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.569249 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.608879 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.626959 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.617747 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.568773 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.683665 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.669923 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.674287 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.679759 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0394 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6301 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Train loss: 0.528321 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.604966 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.528187 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.431266 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.850078 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.491796 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.527040 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.663708 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.571021 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.697827 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.681133 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.661242 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.674196 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0397 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6375 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Train loss: 0.458821 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.643688 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.621286 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.573450 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.733631 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.705433 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.673150 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.493199 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.519992 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.568904 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.642799 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.701333 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.738533 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0396 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6322 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Train loss: 0.619856 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.779208 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.638854 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.554014 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.576725 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.572875 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.662811 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.525325 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.661350 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.622824 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.648980 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.626661 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.640894 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0397 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6259 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Train loss: 0.647719 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.617717 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.561271 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.544277 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.645845 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.522540 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.747124 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.615677 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.516640 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.576773 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.549416 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.583942 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.592427 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0393 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6315 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Train loss: 0.625292 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.645158 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.641670 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.635930 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.657415 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.578382 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.590405 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.654922 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.676051 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.463253 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.609634 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.658971 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.716513 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0395 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6303 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Train loss: 0.766265 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.593425 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.534699 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.600495 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.563603 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.678996 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.604595 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.612222 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.663626 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.585196 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.523496 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.535064 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.717396 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0393 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6296 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Train loss: 0.604625 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.565550 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.729527 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.721212 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.631273 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.497971 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.724251 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.589630 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.525361 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.624851 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.539719 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.644091 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.613760 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0392 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6216 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Train loss: 0.611741 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.566537 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.649079 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.742679 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.730731 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.606524 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.668595 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.633483 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.672041 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.691223 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.603716 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.573495 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.692986 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0395 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6284 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Train loss: 0.616393 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.622941 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.698358 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.681728 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.592655 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.627895 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.578461 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.699219 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.569310 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.652499 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.599190 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.613842 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.605534 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0392 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6319 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Train loss: 0.702550 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.721273 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.598855 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.588339 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.600629 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.672406 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.677037 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.550603 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.546008 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.580545 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.668601 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.664812 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.791726 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0393 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6266 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Train loss: 0.566221 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.575218 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.670216 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.541553 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.682703 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.749429 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.711025 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.572951 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.597454 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.677284 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.636060 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.665446 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.637195 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0393 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6326 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Train loss: 0.535965 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.570832 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.521218 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.633526 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.599961 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.896184 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.530756 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.707247 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.667771 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.575829 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.639171 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.640940 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.668356 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0390 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6332 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Train loss: 0.633032 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.671144 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.614002 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.573130 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.636221 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.582879 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.492819 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.628863 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.789741 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.579025 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.660271 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.606754 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.568852 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0391 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6285 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Train loss: 0.535097 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.668633 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.618371 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.537692 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.520073 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.579478 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.715965 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.553736 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.638159 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.578588 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.571944 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.660689 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.608940 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0391 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6275 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Train loss: 0.716289 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.549923 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.647924 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.637020 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.537219 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.515699 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.633559 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.654555 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.611786 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.566446 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.580492 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.625395 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.782672 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0388 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6274 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Train loss: 0.623930 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.644705 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.563259 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.750339 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.626097 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.649443 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.653062 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.628531 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.633350 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.635484 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.612410 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.661578 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.671700 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0389 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6230 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Train loss: 0.675170 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.571115 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.581736 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.747282 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.588991 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.644076 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.635662 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.685562 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.703752 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.706023 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.720852 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.742873 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.616145 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 100/608 (16.45%), Avg Train loss: 0.0390 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6174 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Train loss: 0.695744 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.535167 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.563692 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.605242 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.571644 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.480353 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.586355 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.631501 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.537950 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.568280 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.720018 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.685657 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.594927 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 99/608 (16.28%), Avg Train loss: 0.0388 \n",
            "\n",
            "Test Accuracy: 101/152 (66.45%), Avg Test loss: 0.6133 \n",
            "\n",
            "Done!\n",
            "Max accuracy: 66.44736842105263\n"
          ]
        }
      ],
      "source": [
        "## dropout with probability 0.25\n",
        "\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "NuZls3tpOPGw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuZls3tpOPGw",
        "outputId": "0a6f03ee-6ccf-4242-e19c-42929cea0787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.75, inplace=False)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (4): Dropout(p=0.75, inplace=False)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
            "    (7): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ],
      "source": [
        "## dropout with probability 0.75\n",
        "class NeuralNetwork(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.75),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.75),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "id": "EfByxulzO55S",
      "metadata": {
        "id": "EfByxulzO55S"
      },
      "outputs": [],
      "source": [
        "## dropout with probability 0.75\n",
        "# define training and testing loops\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jUC5mb8aO_zP",
      "metadata": {
        "id": "jUC5mb8aO_zP"
      },
      "outputs": [],
      "source": [
        "## dropout with probability 0.75\n",
        "# Run model for epochs\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Optimizer Tuning (RMSprop)\n",
        "# dropout with probability 0.5\n",
        "class NeuralNetwork(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxDV47ocsvJv",
        "outputId": "bde59f4e-7d60-43ea-db27-a79cde1ccf36"
      },
      "id": "zxDV47ocsvJv",
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=7, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.5, inplace=False)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
            "    (7): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D2MD90EWPJ6K",
      "metadata": {
        "id": "D2MD90EWPJ6K"
      },
      "outputs": [],
      "source": [
        "## Step 2 \n",
        "# Optimizer Tuning (RMSprop)\n",
        "# dropout with probability 0.5\n",
        "# Run model for epochs\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.01)    # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Optimizer Tuning (Adam)\n",
        "# dropout with probability 0.5\n",
        "# Run model for epochs\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ],
      "metadata": {
        "id": "UIBlo59b4vX7"
      },
      "id": "UIBlo59b4vX7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VyClKkeHSfi4",
      "metadata": {
        "id": "VyClKkeHSfi4"
      },
      "outputs": [],
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (LeakyReLU, slope = 0.02)\n",
        "# dropout with probability 0.5\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(0.02),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(0.02),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "XNXPKTLVTozq",
      "metadata": {
        "id": "XNXPKTLVTozq"
      },
      "outputs": [],
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (LeakyReLU, slope = 0.02)\n",
        "# dropout with probability 0.5\n",
        "# define training and testing loops\n",
        "# define training and testing loops\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hg7b6lImTzPr",
      "metadata": {
        "id": "hg7b6lImTzPr"
      },
      "outputs": [],
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (LeakyReLU, slope = 0.02)\n",
        "# dropout with probability 0.5\n",
        "# Run model for epochs\n",
        "\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UkViPeEAUHbZ",
      "metadata": {
        "id": "UkViPeEAUHbZ"
      },
      "outputs": [],
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ELU)\n",
        "# dropout with probability 0.5\n",
        "class NeuralNetwork(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "s-3lb083UT77",
      "metadata": {
        "id": "s-3lb083UT77"
      },
      "outputs": [],
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ELU)\n",
        "# dropout with probability 0.5\n",
        "# define training and testing loops\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SMOQ0f_gUzL1",
      "metadata": {
        "id": "SMOQ0f_gUzL1"
      },
      "outputs": [],
      "source": [
        "# Step 2 \n",
        "## Activation Function Tuning (ELU)\n",
        "## dropout with probability 0.5\n",
        "# Run model for epochs\n",
        "\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - Xavier\n",
        "# Optimizer - SGD\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # Initialize linear layers with Xavier initialization\n",
        "        for layer in self.linear_relu_stack:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)\n",
        "device"
      ],
      "metadata": {
        "id": "HceECnXTzenl"
      },
      "id": "HceECnXTzenl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - Xavier\n",
        "# Optimizer - SGD\n",
        "# define training and testing loops\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "RuuKkD2OSZvc"
      },
      "id": "RuuKkD2OSZvc",
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - Xavier\n",
        "# Optimizer - SGD\n",
        "# Run model for epochs\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "base_train_acc_history, base_test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, base_train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, base_test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVv3JVM3SsRy",
        "outputId": "8cad0cae-61ed-460e-8a2e-47f69f9126b2"
      },
      "id": "eVv3JVM3SsRy",
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Train loss: 0.474543 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.625198 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.520134 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.652712 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.644325 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.364826 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.634248 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.542079 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.653555 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.534763 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.499846 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.530312 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.514199 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0359 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.5999 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Train loss: 0.546887 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.560128 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.567791 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.603724 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.704885 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.644663 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.530637 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.562048 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.641739 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.617023 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.555956 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.482473 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.499549 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0371 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.6017 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Train loss: 0.459398 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.428509 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.475034 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.593321 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.690729 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.428453 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.667293 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.637094 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.642392 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.637316 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.612778 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.467130 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.613687 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0363 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.5775 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Train loss: 0.503576 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.600189 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.554324 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.680250 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.521023 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.619224 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.476503 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.573714 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.418664 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.603632 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.629772 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.649997 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.640412 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0360 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.5721 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Train loss: 0.471499 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.727450 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.517000 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.600808 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.567657 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.557964 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.573682 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.551584 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.583928 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.496146 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.677026 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.604805 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.488923 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 101/608 (16.61%), Avg Train loss: 0.0362 \n",
            "\n",
            "Test Accuracy: 100/152 (65.79%), Avg Test loss: 0.6028 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Train loss: 0.637939 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.613519 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.730060 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.526157 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.578346 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.544260 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.660684 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.536677 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.519134 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.648286 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.448852 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.730576 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.472435 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0362 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5768 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Train loss: 0.690300 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.436177 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.654824 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.552099 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.703244 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.431018 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.445557 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.511326 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.598037 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.595601 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.526476 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.695518 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.534130 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0358 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5611 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Train loss: 0.652115 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.523265 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.511446 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.680414 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.405124 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.465091 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.444173 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.644200 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.481279 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.582955 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.644403 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.665504 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.529824 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0351 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.5840 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Train loss: 0.548874 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.538485 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.561078 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.620364 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.693368 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.470977 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.404606 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.486500 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.660968 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.528775 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.641578 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.404673 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.513772 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0354 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.5774 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Train loss: 0.531964 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.666130 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.531477 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.612553 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.485777 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.572465 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.557564 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.469321 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.724072 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.569896 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.499255 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.623821 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.482286 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0354 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.5816 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "Train loss: 0.675742 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.520431 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.438495 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.564686 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.476715 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.732384 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.526264 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.557093 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.570868 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.422114 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.493400 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.685435 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.531710 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0350 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5672 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "Train loss: 0.621046 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.365536 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.651216 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.543426 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.623634 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.512927 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.423508 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.306764 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.492800 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.493162 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.686991 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.765431 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.734545 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0356 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5802 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "Train loss: 0.549803 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.662240 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.539813 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.474364 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.596302 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.616357 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.744171 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.353896 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.566413 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.599351 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.410131 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.600449 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.569644 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0352 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5476 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "Train loss: 0.546297 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.502252 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.632556 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.517045 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.334992 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.695807 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.528204 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.643416 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.614178 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.634818 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.493578 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.651046 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.494074 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0348 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5476 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "Train loss: 0.620649 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.470984 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.494051 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.443721 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.500938 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.419580 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.616689 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.557911 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.612104 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.665144 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.532906 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.444180 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.526066 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0349 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.6022 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "Train loss: 0.561784 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.726645 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.612731 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.495977 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.471888 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.611855 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.566581 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.498159 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.473534 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.691382 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.439727 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.644896 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.445981 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 103/608 (16.94%), Avg Train loss: 0.0343 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5854 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "Train loss: 0.669137 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.576842 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.383450 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.817644 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.750797 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.508637 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.525611 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.462157 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.538559 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.561844 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.455868 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.531836 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.497798 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0344 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5782 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "Train loss: 0.615031 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.577257 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.627044 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.572174 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.689585 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.521303 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.791956 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.457693 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.849742 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.619618 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.502211 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.465996 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.374711 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0347 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5717 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "Train loss: 0.430108 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.511430 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.554466 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.657318 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.484337 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.714159 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.575857 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.572880 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.532502 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.585707 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.468511 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.452826 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.491782 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0335 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5798 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "Train loss: 0.266408 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.584480 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.471380 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.548177 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.537749 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.720463 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.506660 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.465546 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.522016 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.578341 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.571837 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.517031 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.498180 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0341 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5391 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "Train loss: 0.641219 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.495443 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.541422 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.800860 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.509305 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.706673 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.591059 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.598634 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.589272 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.382037 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.488572 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.473256 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.610352 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0340 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.5951 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "Train loss: 0.514679 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.600137 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.690159 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.564525 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.444024 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.444289 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.551079 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.485676 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.540628 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.543505 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.591220 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.624466 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.515045 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 95/608 (15.62%), Avg Train loss: 0.0341 \n",
            "\n",
            "Test Accuracy: 103/152 (67.76%), Avg Test loss: 0.5770 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "Train loss: 0.692519 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.543145 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.460509 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.597126 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.538865 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.609073 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.434452 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.455758 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.578573 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.549635 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.598707 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.520997 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.704282 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0337 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5615 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "Train loss: 0.686284 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.364862 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.615769 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.325555 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.691126 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.655761 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.437708 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.662718 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.335973 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.688204 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.590423 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.585178 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.467230 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 98/608 (16.12%), Avg Train loss: 0.0348 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5643 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "Train loss: 0.514054 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.706179 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.511669 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.416816 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.547640 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.453051 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.482037 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.438829 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.545536 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.547127 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.477729 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.403030 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.454537 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0334 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5747 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "Train loss: 0.484469 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.252623 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.534539 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.342485 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.668209 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.538840 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.478623 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.376702 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.461913 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.586762 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.453348 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.440305 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.438933 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0334 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5683 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "Train loss: 0.591506 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.431562 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.415796 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.507005 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.709956 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.433372 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.631294 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.404694 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.503857 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.526024 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.483140 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.552471 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.501101 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0327 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5381 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "Train loss: 0.436522 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.556951 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.531880 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.473950 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.463433 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.533171 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.516473 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.639078 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.929098 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.476734 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.633354 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.821830 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.625147 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 108/608 (17.76%), Avg Train loss: 0.0339 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5457 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "Train loss: 0.529369 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.621404 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.394472 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.491367 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.480624 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.612860 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.375803 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.566762 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.471887 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.528502 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.523657 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.621321 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.536715 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0337 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5916 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "Train loss: 0.613243 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.547252 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.438818 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.654036 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.579576 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.455107 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.565326 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.534627 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.538397 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.426352 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.478423 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.740433 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.560955 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0335 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5240 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "Train loss: 0.480083 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.490506 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.663661 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.538648 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.519640 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.527740 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.525245 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.624941 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.439534 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.478845 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.408719 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.424200 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.464008 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0326 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5557 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "Train loss: 0.470924 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.447868 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.568483 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.453878 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.735691 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.517845 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.435971 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.439232 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.406624 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.597730 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.466141 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.436048 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.562917 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0329 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5440 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "Train loss: 0.381851 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.489455 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.457068 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.553213 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.720479 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.534899 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.637105 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.526089 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.633776 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.607284 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.640512 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.722082 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.722928 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0337 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5263 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "Train loss: 0.421391 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.530679 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.495571 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.717033 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.493637 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.442533 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.514402 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.665680 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.449932 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.627788 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.390573 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.469494 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.458541 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0327 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5759 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "Train loss: 0.440058 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.601181 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.572516 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.431799 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.349770 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.344810 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.524668 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.412311 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.484760 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.439380 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.730969 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.458698 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.436331 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0323 \n",
            "\n",
            "Test Accuracy: 116/152 (76.32%), Avg Test loss: 0.5276 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "Train loss: 0.649091 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.497069 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.505127 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.595960 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.385092 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.667518 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.509273 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.390700 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.463750 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.560750 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.456075 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.587267 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.423577 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0327 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5461 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "Train loss: 0.525820 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.617443 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.494361 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.625370 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.591211 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.483191 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.542491 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.489291 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.359932 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.441582 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.517909 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.603065 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.397900 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0320 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5701 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "Train loss: 0.396871 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.585870 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.316465 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.424340 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.459352 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.881138 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.328745 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.580011 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.511731 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.522971 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.580759 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.592364 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.388336 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0325 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5335 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "Train loss: 0.724515 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.565174 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.514887 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.603456 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.554893 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.415662 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.548815 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.444924 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.494981 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.559362 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.474917 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.485761 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.598072 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 97/608 (15.95%), Avg Train loss: 0.0330 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5594 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "Train loss: 0.611714 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.648419 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.600534 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.578420 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.506373 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.590465 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.473370 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.615291 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.439304 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.578002 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.519322 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.706084 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.505541 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0323 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5497 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "Train loss: 0.610732 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.825876 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.526341 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.364475 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.555669 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.651082 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.306176 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.681849 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.533287 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.443827 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.507103 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.516808 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.341435 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0323 \n",
            "\n",
            "Test Accuracy: 102/152 (67.11%), Avg Test loss: 0.5705 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "Train loss: 0.554051 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.628373 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.832764 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.565211 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.357337 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.571405 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.538588 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.361750 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.477602 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.514960 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.532083 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.611985 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.480790 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0323 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5392 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "Train loss: 0.388983 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.464509 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.682779 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.606525 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.627133 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.554075 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.559781 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.588785 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.543676 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.483563 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.371910 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.446660 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.635419 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0322 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5451 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "Train loss: 0.527862 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.530392 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.396071 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.464994 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.577590 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.479456 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.564697 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.495594 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.470621 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.564863 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.497555 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.453553 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.514290 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0334 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5644 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "Train loss: 0.338979 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.311690 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.453626 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.509924 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.436126 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.531163 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.507108 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.547085 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.377173 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.507915 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.497454 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.527668 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.325259 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0325 \n",
            "\n",
            "Test Accuracy: 107/152 (70.39%), Avg Test loss: 0.5547 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "Train loss: 0.592396 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.395579 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.529223 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.683301 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.758401 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.429247 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.530004 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.525588 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.590839 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.451904 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.738037 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.448296 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.446417 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0323 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5319 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "Train loss: 0.624191 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.486138 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.317971 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.385963 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.448261 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.354587 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.575139 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.624786 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.504585 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.374736 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.415717 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.726937 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.682539 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0318 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5383 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "Train loss: 0.352245 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.880574 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.460670 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.531158 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.379140 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.631369 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.506552 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.506190 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.545221 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.697327 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.559477 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.598296 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.585738 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0315 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5115 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "Train loss: 0.473512 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.522819 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.393543 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.717399 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.619411 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.493287 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.615202 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.581784 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.482217 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.468769 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.618627 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.577977 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.397035 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0323 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5806 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "Train loss: 0.459160 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.620590 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.618329 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.538068 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.596512 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.682370 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.488171 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.360457 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.457762 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.557242 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.504601 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.431156 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.309712 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0331 \n",
            "\n",
            "Test Accuracy: 105/152 (69.08%), Avg Test loss: 0.5539 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "Train loss: 0.605989 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.513923 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.534669 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.633680 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.343413 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.479794 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.531450 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.504990 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.546957 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.506395 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.704200 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.623509 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.302654 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0325 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5476 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "Train loss: 0.428481 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.360272 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.367227 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.531311 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.956164 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.462966 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.631991 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.314158 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.547302 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.636909 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.568818 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.502165 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.360150 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 77/608 (12.66%), Avg Train loss: 0.0315 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5560 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "Train loss: 0.604392 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.506902 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.750952 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.429106 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.368119 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.314814 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.393826 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.597430 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.287285 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.469757 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.731489 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.622914 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.438564 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0321 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5356 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "Train loss: 0.412145 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.700489 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.653826 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.591142 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.304912 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.469204 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.363170 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.532443 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.568723 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.383897 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.500469 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.679604 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.384124 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0316 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5144 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "Train loss: 0.669403 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.436625 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.397645 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.477176 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.425770 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.635256 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.639458 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.412049 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.670879 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.499635 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.360414 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.537278 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.675297 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0323 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5169 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "Train loss: 0.399012 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.554924 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.309445 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.423144 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.680816 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.602576 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.510015 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.397165 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.445757 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.530050 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.466596 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.474025 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.296891 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0318 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5210 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "Train loss: 0.737637 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.348083 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.429157 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.640573 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.480984 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.620397 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.639060 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.311055 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.370286 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.439537 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.660870 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.549531 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.418672 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 77/608 (12.66%), Avg Train loss: 0.0318 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5436 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "Train loss: 0.440250 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.600149 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.391045 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.407842 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.504582 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.336215 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.384292 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.381392 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.577745 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.475008 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.878855 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.476070 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.393554 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0311 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5449 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "Train loss: 0.564519 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.675372 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.515374 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.546299 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.708705 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.502248 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.458820 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.286817 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.714531 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.410270 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.390249 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.446308 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.750097 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0322 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5278 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "Train loss: 0.462648 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.541887 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.491158 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.394723 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.553797 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.572369 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.369316 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.387623 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.686970 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.589348 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.636914 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.477142 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.389434 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0317 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5615 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "Train loss: 0.458979 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.526216 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.391357 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.580857 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.477939 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.374903 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.385441 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.471440 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.629065 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.563243 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.312444 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.684446 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.515510 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0309 \n",
            "\n",
            "Test Accuracy: 116/152 (76.32%), Avg Test loss: 0.5340 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "Train loss: 0.544450 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.343874 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.592464 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.570471 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.793606 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.728236 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.477084 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.491068 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.631081 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.414004 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.477406 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.467395 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.291830 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0310 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5292 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "Train loss: 0.356071 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.396803 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.776354 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.566596 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.559482 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.690737 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.549104 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.495334 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.397633 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.232913 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.420511 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.561112 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.822151 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0316 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5503 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "Train loss: 0.423577 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.575893 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.431962 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.444814 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.536086 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.586152 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.341573 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.513467 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.528585 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.892907 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.392582 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.287404 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.509710 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 79/608 (12.99%), Avg Train loss: 0.0325 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5351 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "Train loss: 0.319934 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.371173 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.402216 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.511664 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.646175 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.521709 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.635159 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.475908 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.544891 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.596915 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.652320 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.569274 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.531774 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 85/608 (13.98%), Avg Train loss: 0.0311 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5602 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "Train loss: 0.644131 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.573012 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.520671 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.600016 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.456754 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.447454 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.536063 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.584924 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.503187 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.515442 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.281785 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.396579 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.577215 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0312 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5401 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "Train loss: 0.682118 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.579843 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.480143 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.422819 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.342634 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.683689 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.374392 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.426636 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.520548 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.422596 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.353338 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.245843 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.437028 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0315 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5234 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "Train loss: 0.848888 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.509738 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.574461 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.517823 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.573727 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.479884 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.368661 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.631353 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.549136 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.433366 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.415530 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.535146 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.429440 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0320 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5358 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "Train loss: 0.397799 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.898026 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.211542 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.272026 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.456949 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.702396 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.627112 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.391687 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.492777 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.606891 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.443730 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.540577 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.359263 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0319 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5335 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "Train loss: 0.905510 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.553829 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.539205 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.578232 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.389192 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.474914 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.407930 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.487950 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.338580 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.559301 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.448919 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.510762 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.779791 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0314 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5267 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "Train loss: 0.523184 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.465189 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.451221 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.567021 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.472616 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.537611 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.383634 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.579750 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.460818 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.727060 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.661359 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.736597 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.424976 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0311 \n",
            "\n",
            "Test Accuracy: 113/152 (74.34%), Avg Test loss: 0.5270 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "Train loss: 0.498257 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.393746 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.707038 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.446504 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.759760 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.637912 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.411014 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.580811 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.457069 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.437530 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.593176 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.519881 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.428845 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0310 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5603 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "Train loss: 0.496164 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.412765 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.384976 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.605773 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.468447 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.455061 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.378618 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.334143 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.619145 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.400655 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.406889 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.466165 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.543206 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0312 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5437 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "Train loss: 0.686953 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.390636 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.563198 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.390821 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.436271 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.595008 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.599983 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.557833 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.300375 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.553300 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.375277 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.621308 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.436549 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 91/608 (14.97%), Avg Train loss: 0.0324 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5200 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "Train loss: 0.311243 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.577334 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.699697 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.403358 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.254303 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.418941 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.458634 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.348972 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.469151 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.722021 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.583557 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.676455 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.604143 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0311 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5315 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "Train loss: 0.343168 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.659553 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.628357 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.444587 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.460639 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.562288 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.574125 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.546115 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.393029 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.487185 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.582163 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.602382 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.412305 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0304 \n",
            "\n",
            "Test Accuracy: 118/152 (77.63%), Avg Test loss: 0.4862 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "Train loss: 0.521285 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.487201 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.630182 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.545528 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.460556 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.501286 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.453691 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.384206 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.596209 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.420709 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.447925 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.440941 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.401891 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 84/608 (13.82%), Avg Train loss: 0.0303 \n",
            "\n",
            "Test Accuracy: 109/152 (71.71%), Avg Test loss: 0.5429 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "Train loss: 0.441527 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.465451 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.585864 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.368141 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.373055 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.500177 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.731203 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.314801 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.518374 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.468797 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.499661 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.377315 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.804969 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0315 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5225 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "Train loss: 0.569519 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.479657 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.456343 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.708900 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.374642 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.533747 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.539244 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.571027 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.450361 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.431035 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.416014 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.627481 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.363485 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 90/608 (14.80%), Avg Train loss: 0.0319 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5662 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "Train loss: 0.483632 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.502482 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.561277 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.432436 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.329175 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.374172 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.605216 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.592114 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.383910 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.459990 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.390008 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.506373 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.478632 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0315 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5557 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "Train loss: 0.523655 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.429645 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.522560 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.667727 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.595195 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.607998 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.412419 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.403104 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.512666 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.478343 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.747577 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.525582 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.471754 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 81/608 (13.32%), Avg Train loss: 0.0314 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5227 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "Train loss: 0.381504 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.526105 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.578323 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.390612 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.615309 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.553223 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.434626 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.427373 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.473463 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.540359 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.460045 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.459894 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.331277 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0306 \n",
            "\n",
            "Test Accuracy: 117/152 (76.97%), Avg Test loss: 0.5266 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "Train loss: 0.494630 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.353547 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.539238 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.563106 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.372904 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.429437 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.465184 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.371111 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.343534 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.384606 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.344981 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.539364 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.476949 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0315 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5149 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "Train loss: 0.441008 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.871947 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.324688 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.639945 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.595288 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.465026 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.437914 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.400039 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.568910 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.451271 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.278084 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.568601 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.378890 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 78/608 (12.83%), Avg Train loss: 0.0310 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5246 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "Train loss: 0.576590 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.498416 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.366665 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.600212 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.523802 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.391447 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.454226 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.811582 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.652113 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.469540 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.711990 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.352806 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.327005 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 88/608 (14.47%), Avg Train loss: 0.0313 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5110 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "Train loss: 0.480531 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.438663 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.372755 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.370191 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.431336 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.453823 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.280031 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.289435 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.602204 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.413920 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.476564 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.617101 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.529936 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 78/608 (12.83%), Avg Train loss: 0.0305 \n",
            "\n",
            "Test Accuracy: 115/152 (75.66%), Avg Test loss: 0.5328 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "Train loss: 0.376280 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.242195 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.354239 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.332397 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.866741 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.433177 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.485888 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.383709 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.775418 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.638458 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.353671 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.541797 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.356072 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 92/608 (15.13%), Avg Train loss: 0.0304 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5649 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "Train loss: 0.641750 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.581549 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.582310 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.607336 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.444766 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.422449 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.362106 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.478538 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.386019 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.725837 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.456763 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.431223 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.583732 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 89/608 (14.64%), Avg Train loss: 0.0312 \n",
            "\n",
            "Test Accuracy: 104/152 (68.42%), Avg Test loss: 0.5824 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "Train loss: 0.374582 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.272416 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.371374 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.712597 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.455258 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.682021 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.511820 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.467045 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.530723 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.493662 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.406346 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.494526 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.537152 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0316 \n",
            "\n",
            "Test Accuracy: 106/152 (69.74%), Avg Test loss: 0.5484 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "Train loss: 0.504276 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.426048 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.324442 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.352248 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.401216 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.658442 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.467569 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.592138 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.442632 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.499426 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.463872 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.470187 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.531184 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 96/608 (15.79%), Avg Train loss: 0.0311 \n",
            "\n",
            "Test Accuracy: 108/152 (71.05%), Avg Test loss: 0.5514 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "Train loss: 0.474046 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.490915 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.385955 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.496673 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.548324 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.754764 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.467605 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.347642 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.565321 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.635654 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.406408 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.383029 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.401382 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 86/608 (14.14%), Avg Train loss: 0.0308 \n",
            "\n",
            "Test Accuracy: 119/152 (78.29%), Avg Test loss: 0.5090 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "Train loss: 0.573831 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.603065 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.615287 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.374319 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.487874 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.585344 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.586065 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.389080 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.494836 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.593223 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.488060 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.236184 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.325111 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0312 \n",
            "\n",
            "Test Accuracy: 118/152 (77.63%), Avg Test loss: 0.5209 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "Train loss: 0.457653 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.527721 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.401262 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.607449 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.483517 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.429003 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.499792 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.279096 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.408365 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.419825 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.730879 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.578268 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.715976 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 82/608 (13.49%), Avg Train loss: 0.0311 \n",
            "\n",
            "Test Accuracy: 110/152 (72.37%), Avg Test loss: 0.5554 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "Train loss: 0.519783 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.425947 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.473526 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.444429 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.389827 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.361688 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.410139 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.451508 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.689149 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.636094 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.617069 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.491311 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.468704 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 94/608 (15.46%), Avg Train loss: 0.0309 \n",
            "\n",
            "Test Accuracy: 117/152 (76.97%), Avg Test loss: 0.5072 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "Train loss: 0.341355 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.683557 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.600600 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.517962 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.439229 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.480147 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.479169 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.326187 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.377668 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.605386 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.516539 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.553349 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.483392 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 93/608 (15.30%), Avg Train loss: 0.0314 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5193 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "Train loss: 0.484789 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.349993 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.453152 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.531572 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.412393 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.287458 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.570240 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.450284 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.392652 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.491123 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.367248 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.612616 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.312515 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0302 \n",
            "\n",
            "Test Accuracy: 111/152 (73.03%), Avg Test loss: 0.5331 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "Train loss: 0.352473 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.551104 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.429744 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.212836 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.463730 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.627295 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.501202 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.468866 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.306734 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.604807 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.680970 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.442288 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.413958 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 87/608 (14.31%), Avg Train loss: 0.0300 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5469 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "Train loss: 0.460377 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.460780 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.399928 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.462263 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.426741 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.489318 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.518195 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.619290 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.505729 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.446881 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.544618 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.722369 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.396042 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 83/608 (13.65%), Avg Train loss: 0.0309 \n",
            "\n",
            "Test Accuracy: 114/152 (75.00%), Avg Test loss: 0.5344 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "Train loss: 0.656062 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.557675 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.432829 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.557067 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.533433 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.368247 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.780346 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.410065 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.194334 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.467741 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.333087 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.548681 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.387410 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 76/608 (12.50%), Avg Train loss: 0.0315 \n",
            "\n",
            "Test Accuracy: 112/152 (73.68%), Avg Test loss: 0.5291 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "Train loss: 0.784259 in Batch:  0  [   16/  608]\n",
            "Train loss: 0.461689 in Batch:  3  [   64/  608]\n",
            "Train loss: 0.440194 in Batch:  6  [  112/  608]\n",
            "Train loss: 0.505097 in Batch:  9  [  160/  608]\n",
            "Train loss: 0.618723 in Batch: 12  [  208/  608]\n",
            "Train loss: 0.868564 in Batch: 15  [  256/  608]\n",
            "Train loss: 0.501544 in Batch: 18  [  304/  608]\n",
            "Train loss: 0.264174 in Batch: 21  [  352/  608]\n",
            "Train loss: 0.594378 in Batch: 24  [  400/  608]\n",
            "Train loss: 0.486312 in Batch: 27  [  448/  608]\n",
            "Train loss: 0.761458 in Batch: 30  [  496/  608]\n",
            "Train loss: 0.485467 in Batch: 33  [  544/  608]\n",
            "Train loss: 0.359936 in Batch: 36  [  592/  608]\n",
            "Train Accuracy: 78/608 (12.83%), Avg Train loss: 0.0310 \n",
            "\n",
            "Test Accuracy: 119/152 (78.29%), Avg Test loss: 0.5343 \n",
            "\n",
            "Done!\n",
            "Max accuracy: 78.28947368421052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2\n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - kaiming\n",
        "# Optimizer - SGD\n",
        "import torch.nn.init as init\n",
        "class NeuralNetwork(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initialize linear layers with kaiming initialization\n",
        "        for layer in self.linear_relu_stack:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "-ed4k6IEUjyP"
      },
      "id": "-ed4k6IEUjyP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - kaiming\n",
        "# define training and testing loops\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "9-_U_8kBUnVt"
      },
      "id": "9-_U_8kBUnVt",
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - kaiming\n",
        "# Optimizer - SGD\n",
        "# Run model for epochs\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ],
      "metadata": {
        "id": "Jy22ulsBUpC3"
      },
      "id": "Jy22ulsBUpC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2\n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - Normal\n",
        "# Optimizer - SGD\n",
        "import torch.nn.init as init\n",
        "class NeuralNetwork(nn.Module):  \n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initialize linear layers with Normal initialization\n",
        "        for layer in self.linear_relu_stack:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.normal_(layer.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "oBB9e6CGWACE"
      },
      "id": "oBB9e6CGWACE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - normal\n",
        "# Optimizer - SGD\n",
        "# define training and testing loops\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "OEJgD_xvWkNr"
      },
      "id": "OEJgD_xvWkNr",
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2 \n",
        "# Activation Function Tuning (ReLU)\n",
        "# dropout with probability 0.5\n",
        "# Initializer - normal\n",
        "# Optimizer - SGD\n",
        "# Run model for epochs\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ],
      "metadata": {
        "id": "C6ZZ1mIEWoGp"
      },
      "id": "C6ZZ1mIEWoGp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3 \n",
        "base model is Dropout\t0.5, Optimizer\tSGD, Activation Function\tReLU, \n",
        "Initializer\tXavier\n",
        "\n",
        "Choosing 'learning rate scheduler' training optimization method and adding it to base model"
      ],
      "metadata": {
        "id": "Lb-hgQXS6wVK"
      },
      "id": "Lb-hgQXS6wVK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9x8PTY7Vb02",
      "metadata": {
        "id": "c9x8PTY7Vb02"
      },
      "outputs": [],
      "source": [
        "## STEP 3 \n",
        "# base model is Dropout\t0.5 \n",
        "# Optimizer\tSGD \n",
        "# Activation Function\tReLU \n",
        "# Initializer\tXavier\n",
        "\n",
        "# a) Choosing 'learning rate scheduler' training optimization method and adding it to base model\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(7, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        # Initialize linear layers with Xavier initialization\n",
        "        for layer in self.linear_relu_stack:\n",
        "          if isinstance(layer, nn.Linear):\n",
        "            init.xavier_uniform_(layer.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 3 \n",
        "# base model is Dropout\t0.5 \n",
        "# Optimizer\tSGD \n",
        "# Activation Function\tReLU \n",
        "# Initializer\tXavier\n",
        "\n",
        "# a) Choosing 'learning rate scheduler' training optimization method and adding it to base model\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "1FB4MnVX-FfI"
      },
      "id": "1FB4MnVX-FfI",
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdebiOdfRnSr"
      },
      "outputs": [],
      "source": [
        "# base model is Dropout\t0.5 \n",
        "# Optimizer\tSGD \n",
        "# Activation Function\tReLU \n",
        "# Initializer\tXavier\n",
        "\n",
        "# a) Choosing 'learning rate scheduler' training optimization method and adding it to base model\n",
        "\n",
        "# Run model for epochs\n",
        "\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ],
      "id": "jdebiOdfRnSr"
    },
    {
      "cell_type": "code",
      "source": [
        "patience = 5\n",
        "global best_loss \n",
        "best_loss = float('inf')\n",
        "counter = 0"
      ],
      "metadata": {
        "id": "7pKE3dw9HR65"
      },
      "id": "7pKE3dw9HR65",
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 3 \n",
        "# base model is Dropout\t0.5 \n",
        "# Optimizer\tSGD \n",
        "# Activation Function\tReLU \n",
        "# Initializer\tXavier\n",
        "\n",
        "# a) Choosing 'early stopping' training optimization method and adding it to base model\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                            # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history, best_loss):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "    \n",
        "    # Check if validation loss has improved\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "\n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "OijL-YbpA2C-"
      },
      "id": "OijL-YbpA2C-",
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base model is Dropout\t0.5 \n",
        "# Optimizer\tSGD \n",
        "# Activation Function\tReLU \n",
        "# Initializer\tXavier\n",
        "\n",
        "# a) Choosing 'learning rate scheduler' training optimization method and adding it to base model\n",
        "\n",
        "# Run model for epochs\n",
        "# Define early stopping criteria\n",
        "\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "train_acc_history, test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history, best_loss)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "    # Check if early stopping criteria has been met\n",
        "    if counter >= patience:\n",
        "        print(f'Early stopping after {t+1} epochs')\n",
        "        break\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ],
      "metadata": {
        "id": "yorwSsxXDZ4H"
      },
      "id": "yorwSsxXDZ4H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## STEP 3 \n",
        "# base model is Dropout\t0.5 \n",
        "# Optimizer\tSGD \n",
        "# Activation Function\tReLU \n",
        "# Initializer\tXavier\n",
        "\n",
        "# a) Choosing 'weight decay' training optimization method and adding it to base model\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, train_losses, train_acc_history):\n",
        "  train_loss, ones = 0.0, 0\n",
        "  pred_out = []                                   # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "  lambda_ = 0.001\n",
        "  size = len(dataloader.dataset)\n",
        "  predictions_train = []                          # to collect predictions\n",
        "  \n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # calculate predictions\n",
        "    X = X.to(next(model.parameters()).dtype)      # Convert X tensor to same data type as model weights and biases\n",
        "    pred = model(X)\n",
        "    predictions_train.append(pred)\n",
        "    pred = pred.to(torch.float64)                 # Convert to float 64, by default it was float32\n",
        "    \n",
        "    # calculate loss\n",
        "    y = y.view(-1, 1)  # reshape to [38, 1]       # Reshape target tensor to have same shape as predicted output tensor\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # weight decay\n",
        "    # Add the weight decay penalty term to the loss\n",
        "    l2_reg = torch.tensor(0.)\n",
        "    for name, param in model.named_parameters():\n",
        "      if 'weight' in name:\n",
        "          l2_reg += torch.norm(param)\n",
        "    loss += lambda_ * l2_reg\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if batch % 3 == 0:                                                          # aesthetics for output\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f'Train loss: {loss:>7f} in Batch: {batch:2d}  [{current:>5d}/{size:>5d}]')\n",
        "\n",
        "  merged_predictions_train_tensor = torch.cat(predictions_train, dim=0)         # merge predictions for all batches\n",
        "  for ele in merged_predictions_train_tensor:                                   # calculate correct predictions                              \n",
        "    if ele >= 0.5:\n",
        "      pred_out.append(1)\n",
        "    else:\n",
        "      pred_out.append(0)\n",
        "\n",
        "  for idx, ele in enumerate(y_test_tensor):\n",
        "     if ele == pred_out[idx]:\n",
        "       ones += 1\n",
        "\n",
        "  accuracy = 100*ones/size\n",
        "  train_acc_history.append(accuracy)\n",
        "\n",
        "  avg_train_loss = train_loss / size\n",
        "  print(f'Train Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Train loss: {avg_train_loss:.4f} \\n')\n",
        "  train_losses.append(avg_train_loss)\n",
        "\n",
        "def test_loop(test_dataset, model, loss_fn, test_losses, test_acc_history):\n",
        "  size = len(test_dataset)\n",
        "  test_loss, correct, ones = 0, 0, 0\n",
        "  predictions = []                              # to collect predictions\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataset:      \n",
        "      # calculate and collect predictions\n",
        "      X = X.to(next(model.parameters()).dtype)  # Convert X tensor to same data type as model weights and biases\n",
        "      pred = model(X)\n",
        "      # print('testing pred:', pred)                               \n",
        "      predictions.append(pred)          \n",
        "      pred = pred.to(torch.float64)             # Convert to float 64, by default it was float32\n",
        "\n",
        "      # calculate loss\n",
        "      y = y.view(-1)                            # Reshape target tensor to have same shape as predicted output tensor\n",
        "      loss = loss_fn(pred, y)\n",
        "      test_loss += loss.item()\n",
        "    \n",
        "    avg_test_loss = (test_loss / size)          # test loss\n",
        "    pred_out = []                               # list that will contain 0 or 1 based on incorrect/correct prediction\n",
        "\n",
        "    for ele in predictions:                     # calculate correct predictions\n",
        "      if ele >= 0.5:\n",
        "        pred_out.append(1)\n",
        "      else:\n",
        "        pred_out.append(0)\n",
        "\n",
        "    for idx, ele in enumerate(y_test_tensor):\n",
        "      if ele == pred_out[idx]:\n",
        "        ones += 1\n",
        "    \n",
        "    accuracy = 100*ones/size\n",
        "    test_acc_history.append(accuracy)\n",
        "\n",
        "    # store for comparision in confusion matrix\n",
        "    global y_pred\n",
        "    y_pred = pred_out\n",
        "    print(f'Test Accuracy: {ones}/{size} ({accuracy:.2f}%), Avg Test loss: {avg_test_loss:.4f} \\n')\n",
        "     \n",
        "  test_losses.append(avg_test_loss)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "BGfeGHcJJfuQ"
      },
      "id": "BGfeGHcJJfuQ",
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base model is Dropout\t0.5 \n",
        "# Optimizer\tSGD \n",
        "# Activation Function\tReLU \n",
        "# Initializer\tXavier\n",
        "\n",
        "# a) Choosing 'weight decay' training optimization method and adding it to base model\n",
        "\n",
        "# Run model for epochs\n",
        "\n",
        "epochs, max_acc = 100, 0\n",
        "train_losses, test_losses = [], []                        # Initialize lists to store losses\n",
        "best_train_acc_history, best_test_acc_history = [], []              # Initialize lists to store accuracy values\n",
        "loss_fn = nn.BCELoss()                                    # loss function\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # optimizer\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, train_losses, best_train_acc_history)\n",
        "    accuracy = test_loop(test_dataset, model, loss_fn, test_losses, best_test_acc_history)\n",
        "    if accuracy > max_acc:                                # store max accuracy for testing\n",
        "      max_acc = accuracy\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Done!\")\n",
        "print(f'Max accuracy: {max_acc}')"
      ],
      "metadata": {
        "id": "_qpdiTrJKWh_"
      },
      "id": "_qpdiTrJKWh_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy curve\n",
        "plt.plot(base_train_acc_history, label='base_train_acc_history')\n",
        "plt.plot(best_train_acc_history, label='best_train_acc_history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "j-nUdRvXvauX",
        "outputId": "e799e108-4f83-4c8a-bf62-62f9139df723"
      },
      "id": "j-nUdRvXvauX",
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy curve\n",
        "plt.plot(base_test_acc_history, label='base_test_acc_history')\n",
        "plt.plot(best_test_acc_history, label='best_test_acc_history')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "PmZUIxaPvY1n",
        "outputId": "0e33ed3c-7441-4795-c5da-bb449fcc339f"
      },
      "id": "PmZUIxaPvY1n",
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PlskE1hushzN",
      "metadata": {
        "id": "PlskE1hushzN"
      },
      "outputs": [],
      "source": [
        "# References \n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "# https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
        "# https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "# https://www.youtube.com/watch?v=jF43_wj_DCQ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}